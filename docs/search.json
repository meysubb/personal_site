[
  {
    "objectID": "posts/march_madness/index.html",
    "href": "posts/march_madness/index.html",
    "title": "College Shiny Apps",
    "section": "",
    "text": "NCAA Shiny Apps"
  },
  {
    "objectID": "posts/march_madness/index.html#march-madness",
    "href": "posts/march_madness/index.html#march-madness",
    "title": "College Shiny Apps",
    "section": "March Madness",
    "text": "March Madness\nFor those of you that love filling out NCAA brackets, have you ever struggled to fill out a bracket? Unless you religiously watch college basketball, there is a high chance that you haven’t seen every team play. It is even more difficult to predict matchups continuoulsy compare stats between set matchups and potential matchups.\nI wanted to avoid constantly jumping from website to website. So I went ahead and spent a little time developing an application, that allows team-team comparison for the tournament this year. This application has data for all tournament teams and will allow you to cross-compare teams and players.\nYou can compare teams based on different stats, and even look at individual players or compare the team as a whole.\n2019/2018: Create a similarity index for teams too 2017: Data updated for games played until 3/15/2017.\n2016: Additionally, I’ve got every team’s schedule (except for games played on 3/13/2016) and a separate tab that has a picture of the current bracket.\n\nApplication Links\n2018-19: NCAA March Madness\n  \n2017-18: NCAA March Madness\n2016-17: NCAA March Madness\n2015-16: NCAA March Madness\nNote: Older versions are not hosted anymore, see github for code.\n\n\nSource Code - March Madness\nShiny Apps Source Code"
  },
  {
    "objectID": "posts/march_madness/index.html#college-world-series",
    "href": "posts/march_madness/index.html#college-world-series",
    "title": "College Shiny Apps",
    "section": "College World Series",
    "text": "College World Series\n\nApplication Link\n2019 CWS App\n\n\n\nSource Code - CWS\nShiny Apps Source Code"
  },
  {
    "objectID": "posts/get-started-heroku/index.html",
    "href": "posts/get-started-heroku/index.html",
    "title": "Heroku for Data",
    "section": "",
    "text": "Over the past few months, I’ve found myself relying on Heroku a lot! My two use cases for Heroku are (1) Twitter bot posting about SEC basketball using ncaahoopR and (2) scraping both NCAA baseball and basketball data for use in a few different projects.\n\nIf you find yourself running repetitive processes, I highly recommend using Heroku! I’ll walk through both my projects and some useful tips I’ve learnt.\n\n\nSet-up Heroku\nObviously, if you don’t already have an account with Herkou set one up. It’s free. It’s also helpful to have their CLI set up, the [instructions](https://devcenter.heroku.com/articles/heroku-cli) on their website are pretty straight forward to get you started.\n\nWhatever you push to your git repo is what will also be pushed to heroku after committing to git. But first let’s figure out how to get our app started. You don’t necessarily need to host apps on Heroku but can use it similar to a cron service if you’d like.\n\nA note Heroku is an ephemeral system. What this means is the files you create/scrape will not last long on the server. What you can do in this instance is save to Dropbox/Drive/S3 whatever service you like.\n\n\nheroku login\n\n\nFirst Steps\nBefore creating the application, we need to ensure that the following\n\nSetting up ENV variables\nIf you plan to set up a Twitter bot or scrape data and save to an external drive (like Dropbox). You’ll definitely need to use an API. These API’s will give you tokens. Naturally, there is concern to save these publicly in a git repo. Heroku provides an easily solution to avoid this. They have config vars, essentially environment variables.\n\nThese can be set up via the command line or the GUI interface. For instructions, again heroku does a great job.\n\nThese variables can then be accessed via either R or Python scripts. Just use os.environ['ENV'] or Sys.getenv('ENV'), for python and R respectively.\n\nInitial Setup\nHeroku relies on build-packs to provide the capability to compile different programming languages. The R build-pack is made for a specific stack heroku-16. The following steps are required to get R setup and running.\n\nR\n\nheroku create\nheroku stack:set 'heroku-16'\nheroku buildpacks:set https://github.com/virtualstaticvoid/heroku-buildpack-r.git#heroku-16\n\nPython\nYou don’t really need to do much in terms of setting a stack or build-pack for Python. Heroku handled them on its own, was able to identify python based on scripts.\n\nheroku create\n\n\nRequired Files\n\nPYTHON\nIf you are using Python, expect to include a requirements.txt file and a runtime.txt file. If the runtime.txt file is missing, Heroku will default to Python 3. If you intend to use Python 2, specify a runtime.txt file.\n\nR\nFor R, an init.R file is necessary with details of the packages you will be using. A quick snapshot of my init.R file looks like this:\n\nmy_packages <- c(\"dplyr\",\"ggplot2\",\"rtweet\",\"lubridate\",'rvest','tidyr','devtools')\ninstall_if_missing <- function(p) {\n  if(p %in% rownames(installed.packages())==FALSE){\n    install.packages(p)}\n}\n\ninvisible(sapply(my_packages, install_if_missing))\n\ndev_packages <- c(\"lbenz730/ncaahoopR\",\"jflancer/bigballR\")\n\ndev_install <- function(p){\n  devtools::install_github(p)\n}\n\ninvisible(sapply(dev_packages,dev_install))\n\n\nI’m installing packages from both CRAN as well as github.\n\nBuild APP\nOnce you are done setting all of this up, run this command to get your files up to heroku.\n\ngit push heroku master\n\n\nAt this point, it will start building up your app. Note there are some free limits in terms of size and usage.\n\nHeroku Scheduler\nHere is the magic of getting scripts to run periodically. Heroku has various different add-ons, one of which is Heroku Scheduler.\n\nOnce you add the Scheduler. You go ahead and click on it and provide a bash command to run whatever script you would like.\nR\n\nRscript app/script.R\n\nPython\n\npython app/script.py\n\n\nThen select the frequency in which these should be run in the drop-down and you should be set. For further details on heroku scheduler, see this link\n\nNote: to set up Heroku add-on’s you have to provide cc info. However, they also provide estimates of costs. In my experience small daily tasks (like tweeting or grabbing data) have not run up a bill in the last 3-6 months that I have been using the service.\n\nR and Python Set-up\nI used R, to build a Twitter Bot, focusing on SEC basketball. Maybe next year the content will be more rich. For this year, it was just game information and the WP charts that were automated. All other content was hand curated.\n\nR + Twitter Bot Repo\nPython + NCAA Baseball Data\n\nAs you can see nothing has really changed in the code except for maybe the paths. In all honestly the paths I could use something like os.join, for python, and something similar in R to avoid the whole app directory situation. Heroku put’s all of the code in your repo under the app folder of the dyno."
  },
  {
    "objectID": "posts/clustering_cfb_offense/index.html",
    "href": "posts/clustering_cfb_offense/index.html",
    "title": "Clustering CFB Offensive Styles",
    "section": "",
    "text": "I scraped this data during the second week of CFB and the first week of the NFL. Excuse my delay on getting around to the analysis.\nNaturally, with every new season comes new AP rankings, new potential all-Americans, etc. That is when I decided to start thinking about some trends to look for in College Football. Since style of play (offensively) is one of the hottest topics in the game, why not find a way to look at that."
  },
  {
    "objectID": "posts/clustering_cfb_offense/index.html#win-distribution-by-conference",
    "href": "posts/clustering_cfb_offense/index.html#win-distribution-by-conference",
    "title": "Clustering CFB Offensive Styles",
    "section": "Win Distribution by Conference",
    "text": "Win Distribution by Conference\n\nThe plot above shows the win distribution per conference over the last 3 years."
  },
  {
    "objectID": "posts/clustering_cfb_offense/index.html#style-of-play-clustering-analysis",
    "href": "posts/clustering_cfb_offense/index.html#style-of-play-clustering-analysis",
    "title": "Clustering CFB Offensive Styles",
    "section": "Style of Play (Clustering Analysis)",
    "text": "Style of Play (Clustering Analysis)\nBelow we can see the overview of the offensive data for all NCAA teams from 2013 to 2015. This table will be super useful later on.\n\n\n\n\n\nValue\n\n\nMinimum\n\n\nMaximum\n\n\nMean\n\n\nMedian\n\n\nStdev\n\n\n\n\n\n\nWins\n\n\n0.000\n\n\n14.00\n\n\n6.72\n\n\n7.00\n\n\n3.188\n\n\n\n\nPlays Per Game\n\n\n59.667\n\n\n87.54\n\n\n71.71\n\n\n71.42\n\n\n5.424\n\n\n\n\nYards per Play\n\n\n1.919\n\n\n7.67\n\n\n5.63\n\n\n5.56\n\n\n0.800\n\n\n\n\nYards per Score\n\n\n6.981\n\n\n24.23\n\n\n14.36\n\n\n13.78\n\n\n2.451\n\n\n\n\nTime per Play (Seconds)\n\n\n19.506\n\n\n32.07\n\n\n25.22\n\n\n25.10\n\n\n2.639\n\n\n\n\nTotal Offense Yards (per Game)\n\n\n120.080\n\n\n618.77\n\n\n404.87\n\n\n400.08\n\n\n70.717\n\n\n\n\nOffensive Ratio per Game (Passing Yards/Rushing Yards)\n\n\n0.175\n\n\n12.00\n\n\n1.52\n\n\n1.40\n\n\n0.891\n\n\n\n\n\nAs we all know, College Football is being taken over by the Spread Offense. There are other styles, such as the option and pro style. Let’s see if we can use stats to identify various styles of play. Additionally, I am hoping to see whether style of play corresponds to a higher win percentage. There are various styles of play, option, spread, and pro style offenses. The four main schools of spread offense are: (1) Air Raid, (2) Spread Option, (3) Smashmouth Spread and (4) Pro-style Spread.\nTraditionally speaking different styles are defined by the types of sets they come out of. A pro style offense will traditionally come out in the I-formation or the pistol formation and very rarely line up in shotgun. Whereas a traditional spread team will line up in the shotgun and very rarely anything else. Of course each coach has their own wrinkles.\nThe question was how to mathematically quantify styles of play. Luckily enough, each style of play has distinct characteristics and these can be seen in the stats. For instance, a successful spread team usually runs a high number of plays per game. Spread offenses are also extremely quick and incorporate no-huddles when they catch defenses in the wrong set, this correlates to lower Time of Possession (than Pro-Style offenses) and very short time per play. It is important to note, that traditionally when talking about time and spread offenses, ESPN and other TV broadcasts highlight the time from the end of the previous play to the snap of the next play. Sadly, we don’t have access to such data.\nI chose the following statistics to identify team styles via a clustering analysis. These statistics were deemed to be the best indicators of style. They are as follows:\n* Number of Plays Per Game\n* Yards Per Play\n* Time Per Play\n* Total Offensive Yards Per Game\n* Ratio: (Passing Yards Per Game/Rushing Yards Per Game)\nThe next question is how to use stats that have different units. The stats are scaled using the scale function in R. The scale function centers a set of values by subtracting each value by the mean and dividing by the standard deviation. At this point, we should be able to start running our analysis.\nOne of the most popular clustering methods in data science is K-means clustering. The clustering method partitions the observations into k clusters. Clustering analysis requires that you indicate the number of clusters expected from the data. They have different methods to identify the number of potential clusters. Both the elbow method and gap statistic show that there should be 3 clusters in the data.\n\n\n\n\n\n\n\nCluster_1\n\n\nCluster_2\n\n\nCluster_3\n\n\n\n\n\n\nPlays_Per_Game\n\n\n-0.3968\n\n\n-0.5097\n\n\n0.974\n\n\n\n\nYds_Per_Play\n\n\n-1.1168\n\n\n0.0787\n\n\n0.677\n\n\n\n\nYds_Per_Score\n\n\n1.2454\n\n\n-0.3782\n\n\n-0.359\n\n\n\n\nTime_Per_Play\n\n\n0.0556\n\n\n0.6623\n\n\n-0.942\n\n\n\n\nTot_Off_Yards_Per_Game\n\n\n-1.0539\n\n\n-0.1804\n\n\n0.986\n\n\n\n\nratio\n\n\n0.1809\n\n\n-0.3382\n\n\n0.334\n\n\n\n\n\nThe results from the cluster can be seen above. Mind you the numbers above are scaled. Let’s take a look at each individual cluster and see if we can identify a style of play. In each of these descriptions, I will interpret the results above.\n\nCluster 1\nAlright, let’s make sense of the numbers above. But before we start, lets note that the data-set has a total of 371 observations. This first cluster has 85 observations, that is 23% of the data-set.\nSince the data was scaled before running the cluster analysis, the way these results should be read are that it is (insert number) standard deviations above/below the mean. Example: In Cluster 1, the number of plays per game are -0.39 standard deviations below the mean. What does this mean? The average plays per game for this data-set is 71 plays, so generally teams in this cluster run less plays per game.\nLooking at the remainder of the results, teams don’t gain a lot of yards per play (-1.11 sd below the mean) naturally corresponds to teams needing to gain more yards per score (1.245 sd above the mean). This seems like the most likely cause for having such drastically different numbers.\nTeams lean slightly towards passing the ball more often than rushing, the ratio in this cluster is 0.18 standard deviations above the mean. Lastly teams are a whole standard deviation below the mean for total offensive yards per game.\nSo what does all of this tell us? That teams in this cluster are highly unsuccessful. Another thought could be that they have porous defense, and that is something that will be verified later. I am really not sure what style of play to characterize this cluster as. Based on the table below, we can clearly tell that teams here don’t win much. Ineffective Offense that’s about the best I can do for now. There is some future work to do here. But let’s look at the other clusters first, hopefully the results are better.\n\n\n\n\n\nTeam\n\n\nWins\n\n\nYear\n\n\nPlays_Per_G\n\n\nOffensive_Ratio\n\n\nYards_Per_Score\n\n\nTime_Per_Play\n\n\n\n\n\n\nBoston College\n\n\n3\n\n\n2015\n\n\n62.6\n\n\n0.676\n\n\n6.98\n\n\n28.7\n\n\n\n\nIowa St. \n\n\n2\n\n\n2014\n\n\n76.0\n\n\n2.002\n\n\n16.06\n\n\n22.5\n\n\n\n\nIowa St. \n\n\n3\n\n\n2013\n\n\n75.2\n\n\n1.524\n\n\n14.64\n\n\n24.8\n\n\n\n\nIowa St. \n\n\n3\n\n\n2015\n\n\n75.8\n\n\n1.243\n\n\n16.33\n\n\n25.0\n\n\n\n\nKansas\n\n\n0\n\n\n2015\n\n\n74.8\n\n\n1.936\n\n\n21.67\n\n\n22.7\n\n\n\n\nKansas\n\n\n3\n\n\n2013\n\n\n68.8\n\n\n0.911\n\n\n19.25\n\n\n25.9\n\n\n\n\nKansas\n\n\n3\n\n\n2014\n\n\n70.2\n\n\n1.674\n\n\n18.21\n\n\n25.9\n\n\n\n\nKentucky\n\n\n2\n\n\n2013\n\n\n64.5\n\n\n1.307\n\n\n16.64\n\n\n26.0\n\n\n\n\nNorth Carolina St. \n\n\n3\n\n\n2013\n\n\n78.8\n\n\n1.480\n\n\n17.70\n\n\n24.3\n\n\n\n\nNorthwestern\n\n\n10\n\n\n2015\n\n\n73.2\n\n\n0.735\n\n\n16.77\n\n\n24.4\n\n\n\n\nOregon St. \n\n\n2\n\n\n2015\n\n\n65.8\n\n\n0.897\n\n\n17.71\n\n\n23.9\n\n\n\n\nPurdue\n\n\n1\n\n\n2013\n\n\n62.1\n\n\n3.216\n\n\n17.51\n\n\n26.6\n\n\n\n\nPurdue\n\n\n2\n\n\n2015\n\n\n76.8\n\n\n1.807\n\n\n14.68\n\n\n21.8\n\n\n\n\nPurdue\n\n\n3\n\n\n2014\n\n\n69.7\n\n\n1.192\n\n\n14.48\n\n\n24.9\n\n\n\n\nSyracuse\n\n\n3\n\n\n2014\n\n\n67.2\n\n\n1.263\n\n\n18.63\n\n\n23.8\n\n\n\n\nVanderbilt\n\n\n3\n\n\n2014\n\n\n61.7\n\n\n1.639\n\n\n16.76\n\n\n28.7\n\n\n\n\nVirginia\n\n\n2\n\n\n2013\n\n\n82.9\n\n\n1.352\n\n\n18.61\n\n\n24.1\n\n\n\n\nVirginia Tech\n\n\n8\n\n\n2013\n\n\n71.2\n\n\n1.972\n\n\n15.82\n\n\n28.1\n\n\n\n\nWake Forest\n\n\n3\n\n\n2014\n\n\n64.0\n\n\n4.419\n\n\n14.61\n\n\n26.9\n\n\n\n\nWake Forest\n\n\n3\n\n\n2015\n\n\n69.5\n\n\n2.170\n\n\n19.16\n\n\n26.9\n\n\n\n\n\n\n\nCluster 2\nThe Time Per Play in Cluster 2 is 0.66 standard deviations above the mean for all teams in CFB (13-16). Teams take their time on offense and let the play develop.The offensive ratio is -0.34 standard deviations below the mean, thus showing a bias towards running the ball more often than passing. Keep in mind that the average offensive ratio is 1.5, this leans towards the pass. Yards per score is indicative of the number of big plays a team has. In this case, the yards per score is -0.37 standard deviations below the mean.\nOverall for Cluster 2 (Teams): don’t run a lot of plays, are not big play teams, milk the clock every play, and run the ball more. What style does that sound like to most of you? I would say a pro-style offense. It is worth noting that more teams in this cluster seem to have a slight lean towards running the ball. But based on the total offensive yards, it seems like a very balanced approach.\nAlright well let’s take a look at some of the teams in this cluster. This cluster has about 165 observations out of the total 371. That is about 45% of the data-set.\n\n\n\n\n\nTeam\n\n\nWins\n\n\nYear\n\n\nPlays_Per_G\n\n\nOffensive_Ratio\n\n\nYards_Per_Score\n\n\nTime_Per_Play\n\n\n\n\n\n\nAlabama\n\n\n14\n\n\n2015\n\n\n72.5\n\n\n1.136\n\n\n12.2\n\n\n27.8\n\n\n\n\nArkansas\n\n\n3\n\n\n2013\n\n\n64.6\n\n\n0.712\n\n\n17.3\n\n\n28.3\n\n\n\n\nColorado\n\n\n4\n\n\n2013\n\n\n69.3\n\n\n2.062\n\n\n14.6\n\n\n24.9\n\n\n\n\nFlorida St. \n\n\n13\n\n\n2014\n\n\n69.1\n\n\n2.196\n\n\n13.1\n\n\n25.1\n\n\n\n\nGeorgia Tech\n\n\n3\n\n\n2015\n\n\n64.6\n\n\n0.475\n\n\n12.9\n\n\n28.9\n\n\n\n\nIndiana\n\n\n4\n\n\n2014\n\n\n71.0\n\n\n0.536\n\n\n16.1\n\n\n23.9\n\n\n\n\nIowa\n\n\n12\n\n\n2015\n\n\n66.9\n\n\n1.125\n\n\n12.5\n\n\n28.3\n\n\n\n\nLouisville\n\n\n12\n\n\n2013\n\n\n68.8\n\n\n2.139\n\n\n13.1\n\n\n29.5\n\n\n\n\nMaryland\n\n\n3\n\n\n2015\n\n\n69.1\n\n\n0.868\n\n\n15.2\n\n\n24.0\n\n\n\n\nMichigan St. \n\n\n12\n\n\n2015\n\n\n70.9\n\n\n1.548\n\n\n12.9\n\n\n27.8\n\n\n\n\nMichigan St. \n\n\n13\n\n\n2013\n\n\n71.4\n\n\n1.218\n\n\n13.1\n\n\n28.0\n\n\n\n\nOhio St. \n\n\n12\n\n\n2015\n\n\n68.6\n\n\n0.770\n\n\n12.2\n\n\n25.6\n\n\n\n\nRutgers\n\n\n4\n\n\n2015\n\n\n67.2\n\n\n1.219\n\n\n13.9\n\n\n27.9\n\n\n\n\nSouth Carolina\n\n\n3\n\n\n2015\n\n\n64.5\n\n\n1.341\n\n\n16.5\n\n\n26.5\n\n\n\n\nStanford\n\n\n12\n\n\n2015\n\n\n66.3\n\n\n0.947\n\n\n11.5\n\n\n31.5\n\n\n\n\nSyracuse\n\n\n4\n\n\n2015\n\n\n62.6\n\n\n0.961\n\n\n11.7\n\n\n28.1\n\n\n\n\nTCU\n\n\n4\n\n\n2013\n\n\n68.5\n\n\n1.908\n\n\n13.7\n\n\n26.6\n\n\n\n\nVirginia\n\n\n4\n\n\n2015\n\n\n69.8\n\n\n1.646\n\n\n14.8\n\n\n28.1\n\n\n\n\n\n\n\nCluster 3\nThe third cluster has a 121 observations out of the total 371, that is 32.6% of the data-set.\nTime per play is about a whole standard deviation below the mean. On average, teams in this cluster spend less time per play than others. Yards Per Play is 0.68 standard deviations above the mean, indicating that teams have success with the big play. The Offensive ratio is a third of a standard deviation above the mean. Since the mean for this stat leans towards the pass (1.5), being partially above the mean indicates a strong passing offense.\nOverall for Cluster 3 (Teams): run a lot of plays, gain a lot of yards per play, run quick plays, and pass the ball more. Notice that I didn’t mention anything about how they score the ball (yards gained). The majority of these descriptors suggest these teams to be classified as an up-tempo offense (i.e Spread). Well we know that spread offenses utilize wide screen passes and RPO (Run, Pass options). The quantity of these plays and success of these plays affect the number of yards per score. Or you could have a play-caller like Jake Spavital (ex-TAMU OC, current Cal OC) who calls so many of them that it is always unsuccessful. Essentially, what I am trying to conclude is that depending on the type of spread a team runs, the average yards per score can change drastically.\n\n\n\n\n\nTeam\n\n\nWins\n\n\nYear\n\n\nPlays_Per_G\n\n\nOffensive_Ratio\n\n\nYards_Per_Score\n\n\nTime_Per_Play\n\n\n\n\n\n\nAlabama\n\n\n12\n\n\n2014\n\n\n72.7\n\n\n1.345\n\n\n13.1\n\n\n26.2\n\n\n\n\nAuburn\n\n\n12\n\n\n2013\n\n\n72.4\n\n\n0.527\n\n\n12.7\n\n\n25.2\n\n\n\n\nBaylor\n\n\n11\n\n\n2013\n\n\n82.6\n\n\n1.383\n\n\n11.8\n\n\n19.8\n\n\n\n\nBaylor\n\n\n11\n\n\n2014\n\n\n87.5\n\n\n1.698\n\n\n12.1\n\n\n19.9\n\n\n\n\nCalifornia\n\n\n1\n\n\n2013\n\n\n87.1\n\n\n2.712\n\n\n19.7\n\n\n20.2\n\n\n\n\nClemson\n\n\n11\n\n\n2013\n\n\n79.8\n\n\n1.908\n\n\n12.6\n\n\n20.5\n\n\n\n\nClemson\n\n\n14\n\n\n2015\n\n\n80.5\n\n\n1.307\n\n\n13.4\n\n\n23.8\n\n\n\n\nColorado\n\n\n2\n\n\n2014\n\n\n83.0\n\n\n1.841\n\n\n15.4\n\n\n23.5\n\n\n\n\nFlorida St. \n\n\n14\n\n\n2013\n\n\n67.6\n\n\n1.555\n\n\n10.1\n\n\n26.1\n\n\n\n\nIllinois\n\n\n4\n\n\n2013\n\n\n72.2\n\n\n2.070\n\n\n14.4\n\n\n24.3\n\n\n\n\nMichigan St. \n\n\n11\n\n\n2014\n\n\n76.5\n\n\n1.129\n\n\n11.6\n\n\n27.7\n\n\n\n\nMissouri\n\n\n12\n\n\n2013\n\n\n74.4\n\n\n1.063\n\n\n12.6\n\n\n24.2\n\n\n\n\nNorth Carolina\n\n\n11\n\n\n2015\n\n\n66.9\n\n\n1.170\n\n\n12.0\n\n\n22.7\n\n\n\n\nOhio St. \n\n\n12\n\n\n2013\n\n\n71.6\n\n\n0.659\n\n\n11.3\n\n\n26.3\n\n\n\n\nOhio St. \n\n\n14\n\n\n2014\n\n\n73.3\n\n\n0.934\n\n\n11.4\n\n\n25.8\n\n\n\n\nOklahoma\n\n\n11\n\n\n2015\n\n\n77.9\n\n\n1.388\n\n\n12.2\n\n\n23.6\n\n\n\n\nOregon\n\n\n11\n\n\n2013\n\n\n74.8\n\n\n1.066\n\n\n12.4\n\n\n20.4\n\n\n\n\nOregon\n\n\n13\n\n\n2014\n\n\n74.5\n\n\n1.333\n\n\n12.0\n\n\n21.6\n\n\n\n\nTCU\n\n\n11\n\n\n2015\n\n\n82.9\n\n\n1.613\n\n\n13.4\n\n\n22.7\n\n\n\n\nTCU\n\n\n12\n\n\n2014\n\n\n79.8\n\n\n1.577\n\n\n11.5\n\n\n23.0\n\n\n\n\nTexas Tech\n\n\n4\n\n\n2014\n\n\n76.2\n\n\n2.295\n\n\n16.5\n\n\n20.5\n\n\n\n\nWashington St. \n\n\n3\n\n\n2014\n\n\n84.5\n\n\n12.003\n\n\n16.3\n\n\n22.4\n\n\n\n\nWest Virginia\n\n\n4\n\n\n2013\n\n\n74.3\n\n\n1.764\n\n\n15.6\n\n\n22.9\n\n\n\n\n\n\n\nPlots\nI’m hoping this section helps visualize the styles of play and how they correlate to some of these statistics.\n\nThe further along we move up the x-axis (Passing Offense) teams are classified as Spread teams. Of course it is interesting to see that teams that belong to the Pro-Style can have a tendency to be quite balanced or lean towards the run slightly. All of the other teams seem to fare poorly with running and passing the ball. Fittingly, I have deemed them in the “Ineffective Offense” category.\n\nIn the above plot, the horizontal and vertical lines to highlight the mean values for their respective axes. The left corner is the epitome of a fast pace offense, quick hitting plays and tons of offensive yards per game. Interestingly enough, the pro-style offenses fall in multiple quadrants. In general, it seems the pro-style offenses take quite a bit of time per play, regardless of the number of yards gained per game.\n\nI had to cut off two data points, both belonged to Washington State. The ratio was 8 or higher. Leave it to Mike Leech to run a high-paced passing offense. Where was the running those two years? Who even knows.\nLooking at the rest of this, again the spread offenses tend to lie above the mean for the offensive ratio while the pro-style offenses mostly lie below the mean. It is interesting to note though that there are not a lot of teams below 1. Essentially, passing is required to succeed in CFB even if your focus is the running game.\n\n\nStyle of Play in Conferences\nThe table below shows the distribution of clusters throughout the different conferences. Note I didn’t show Non-Power 5 conferences as it skews the table.\n\n\n\n\n\nConference\n\n\nCluster 1\n\n\nCluster 2\n\n\nCluster 3\n\n\n\n\n\n\nACC\n\n\n10\n\n\n26\n\n\n9\n\n\n\n\nBig-10\n\n\n8\n\n\n27\n\n\n7\n\n\n\n\nBig-12\n\n\n7\n\n\n7\n\n\n16\n\n\n\n\nPac-12\n\n\n2\n\n\n12\n\n\n22\n\n\n\n\nSEC\n\n\n5\n\n\n23\n\n\n14\n\n\n\n\n\nThe plot below shows offensive styles in each conference. For instance, the Big-10 has historically always been a pre-dominantly run first conference. One of the main reasons for this is the weather and the difficulty it poses on extensive passing playbooks.\nAnother important topic to study would be looking if conferences had a shift in the style of play. With the inclusion of Texas A&M and Missouri, the SEC changed its landscape a bit. With a few key hires, all of a sudden the hurry-up offense and spread were now present in the SEC. Something the league had not seen in the past. We all know how adamant Nick Saban and Brett Bielma have been about condemning aspects of the hurry-up offense at SEC media days. Note: Nick Saban and Alabama were marked as pro-style offense for two years and the year they hired Lane Kiffin our analysis indicated that they were a spread offense now.\n\n\n\nConclusions\nTLDR: I used a set of statistics to identify various styles of play in college football. I wanted to look at a couple of things, whether a specific style had a bias to winning, the change in styles in the Power-5 conferences.\nOver the last couple of years, it seems that the SEC and Pac-12 have quite a bit of parity in offensive styles. As expected the Big 10 and Big 12, identify as Pro-Style and Spread offenses respectively. I would have pinned the ACC to have some parity in the offensive styles seen, but to the contrary the majority are pro-style. Also this study once again establishes the dominance of offenses in the Pac-12. Only two teams in the last 3 years fall under the Bad Offenses category. Another shocker is the placement of 5 teams in the SEC under the Bad Offense category.\nA couple of areas of future work is to be able to distinguish between pro-style and option offenses. The KMeans method doesn’t seem to be able to capture that. It could very well be the data I have or the algorithm I am using. There is some additional work to be done on this front, I was thinking of using other techniques. Maybe Gaussian Mixture Models and/or PCA? If any of you have thoughts, shoot me a tweet\n\n\nSource Code\nCode on github.\nData Scraping here."
  },
  {
    "objectID": "posts/tamu_baseball/index.html",
    "href": "posts/tamu_baseball/index.html",
    "title": "TAMU BASEBALL",
    "section": "",
    "text": "In the past, baseball would not have interested me that much, be it professional or college. Needless to say, just like anyone else my interests have shifted over time. I would like to attribute it to a growing number of factors, from having a committee member who loves baseball to an old roommate writing for the Astros blog (SB Nation’s Crawfish boxes, check out his posts especially if you are a fan that belongs to the AL West). Oh yeah, sadly there is a Yankee’s fan that’s in the mix as well. Luckily, I won’t ever considering cheering for any other team but my beloved Texas Rangers. It also helps to keep up with baseball, when your team is constantly ranked in the Top 25. Gig’Em Ags!\nA few months ago, during my job hunt I decided to try my luck and apply for a few sports analytics jobs. I never thought I would get a call back, especially from baseball teams. But I did, and I must say the experience was fantastic. Thanks to that I have decided to share with y’all some projects that I undertake. If you think there are any in particular that might interest me, please do share.\nIf any of you have kept up with college baseball this year, then you have definitely heard a lot about SEC and ACC teams, specifically Texas A&M. The Texas A&M baseball coach, Rob Childress is known for having quality-pitching staffs. Interestingly enough this year it is the Texas A&M offense that has carried the team. The most recent 22-run game against Wake Forest in the regional’s reminded us of the ridiculous offensive prowess that the Aggies possess. Of course, South Carolina reminded everyone that they were just as offensively talented that weekend by putting 23 runs against Rhode Island. Why don’t we take a look at the stats? For those of you curious on how I got a hold of the stats, I have a link to my github repository at the bottom of this page."
  },
  {
    "objectID": "posts/tamu_baseball/index.html#plate-discipline",
    "href": "posts/tamu_baseball/index.html#plate-discipline",
    "title": "TAMU BASEBALL",
    "section": "Plate Discipline",
    "text": "Plate Discipline\nA great hitting team will usually display great plate discipline. There are a host of statistics that can determine plate discipline, but we are limited to the stats recorded by the NCAA. So for this, we will look at strikeout rate (K%) and walk rate (BB%). A higher walk-rate results in the batter reaching base more often. It is a little bit more difficult to gather information from strikeout rate alone. It does however give you some insight on the batter’s ability to make contact. The combination of a high walk-rate and low strike rate suggests that the batter is quite good at determining pitch locations and usually gets contact.\n\n\nNow let’s take a look at two plots, the first plot contains data for players that belong to Top 25 teams (as of the last rankings) and the second plot contains data for only SEC players. The blue lines represent an excellent rating for K% and BB%, a player above the blue line for BB% is deemed excellent while a player below the blue line for K% is also deemed excellent. The red lines depict a poor rating.\nIt is immediately clear that Boomer White and Michael Barash have good plate discipline. This is kind of expected of White, seeing as he won numerous awards. White was the SEC player of the year, a first team All-American, and a semi-finalist for the Golden Spikes award. Barash is a bit more surprising as he was never in the top of the order, and he only had a 0.330 BA, and 0.390 OBP. (For reference White has a 0.392 BA and a 0.468 OBP)"
  },
  {
    "objectID": "posts/tamu_baseball/index.html#run-production",
    "href": "posts/tamu_baseball/index.html#run-production",
    "title": "TAMU BASEBALL",
    "section": "Run Production",
    "text": "Run Production\nOffensive powerhouses are usually not one-dimensional. Instead they tend to score runs by knocking it out of the park, as well as bringing in runs via stolen bases and sacrifice hits.\nIn the following plots, the x-axis represents the number of home runs a team has hit, while the y-axis is a summation of the sacrifice hits and stolen bases. The black, blue and orange lines represent the national, SEC, and Top 25 averages, respectively. The first plot shows all teams in Division I baseball, while the second plot only considers SEC and Top 25 teams.\n\nTeams in the upper right hand region (all points above the intersection of the orange lines) are quality offensive teams. Texas A&M once again distinguishes itself from the pack. Interestingly enough, Coastal Carolina does well in both categories. This might stem from Coastal Carolina allegiance to a non-power 5 conference. But that idea was immediately dispelled with their whopping of LSU and advancement to the CWS.\n\nEven though throughout the year 1/3 of the Top 25 poll consisted of SEC teams, it was not because of offense. In fact, Texas A&M is the only SEC team to be above average in both categories. Louisville and its star-studded lineup seems to have performed much better in the Sac Fly/Bunt and Stolen bases category in comparison to hitting home runs. (Note however that both were above average.)"
  },
  {
    "objectID": "posts/tamu_baseball/index.html#sabermetrics",
    "href": "posts/tamu_baseball/index.html#sabermetrics",
    "title": "TAMU BASEBALL",
    "section": "Sabermetrics",
    "text": "Sabermetrics\nSince Bill James book in 1980, the Majors has adopted various sabermetrics to further assess players. NCAA does not track such data for a variety of reasons from being a college league to not having funding. I do however think it would be possible, they could hire college students to keep track of some of the more advanced statistics for really cheap at each university. There are a few offensive sabermetrics that can be calculated with the available stats provided by the NCAA.\nThe first metric to look at is wOBA (weighted On-Base Average), it is an all inclusive offensive statistic. This metric determines a hitter’s offensive value by placing relative weights on different offensive events. The formula for wOBA is:\n\nThe results for wOBA were plotted against AB and can be seen below. The black line denotes the national average wOBA.\n\nTexas A&M players with an wOBA over 0.4 were highlighted above. Those of you who watched Aggie baseball know that that Moroney did not get hot until the SEC tournament and onwards. The majority of Aggies are above the wOBA average, and once again Boomer White distinguishes himself from the pack.\nA more visual approach to wOBA would be to breakdown the percentage of hit types per player or team. A pie chart is used to compare the NCAA average, SEC average, Top 25 average and Texas A&M’s distribution of hits.\n\nInterestingly enough, the Aggies have higher percentages in all hit types, with the exception of walks and hit by pitch. This also helps explain why the majority of Aggies have a low walk percentage (as seen in the plate discipline plots). Seeing as the Aggies have a high percentage of singles comparatively, they should have a high BABIP (Batting Average on Balls In Play).\n{: .center} \n\nThe dashed lines represent one standard deviation above and below the national average. Most Aggie’s are within one standard deviation above the national average. The truly great hitters are usually above the national average by a whole standard deviation. Only three Aggie hitters are above this mark: Ronnie Gideon, Austin Homan and Boomer White. We have been over the Gideon case, where he had defensive deficiencies and was continuously inconsistent offensively. Austin Homan is quite a surprise, but looking at his stat line puts this in perspective. Homan hit 1 HR all year, brought in 26 RBIs and 56 hits. It seems that he was quite adept at getting on base and putting balls in play. Of course, White shows up once again further solidifying his offensive prowess.\nIn the following tables, we have summarized some statistics. The most recent addition is the wRAA. wRAA is the weighted runs above average, it measures the number of offensive runs a player contributes to their team compared to the average player. The equation to find wRAA is:\n{: .center} \n\nThe first table highlights all players who have above a 25 wRAA, considered to be way above average for collegiate players and anything about 32 or so should be considered elite (for the majors, anything about 40 is considered elite). Even though the BA is sporadic, the wOBA highlights the similarity between these players (all above 0.450). Note the league average was calculated as a conference average, this allows for a better comparison between their counterparts. This does introduce some contention, instead of using the national average, the league average will lead to highlighting big name players in smaller conferences and ignoring big name players in big conferences. Those with lesser competition will all of a sudden be seen as better players. Seth Beer from Clemson, the freshman freak seems to be incredibly valuable. This should bolster his status for the Golden Spikes award.\n\nThe second table shows the most horrendous offensive players. Notice also that they have a high number of ABs, these players must have provided great defense or a limited depth must have kept them in the lineup. Just poor numbers in general.\nFrom the looks of the data, it seems that Texas A&M was indeed well above average offensively. To claim them an offensive powerhouse is a bit of a stretch, they were above average normally but at no point were they dominating a statistics (example: Coastal Carolina).\nAlso, from all of this it seems to make sense as to why so many accolades fell to Boomer White. A player to watch for the future? Seth Beer (This kid is a monster and will probably tear up the collegiate ranks next year. I would be weary of Clemson)."
  },
  {
    "objectID": "posts/tamu_baseball/index.html#mlb-draft",
    "href": "posts/tamu_baseball/index.html#mlb-draft",
    "title": "TAMU BASEBALL",
    "section": "MLB Draft",
    "text": "MLB Draft\nSince the MLB draft was recently (June 9-11), it seems fitting to look at some basic stats.\n\nIn general the players drafted to the MLB have seen significant AB’s have above a 0.4 wOBA and are above average in terms of runs provided to their team (wRAA).\nLastly, we can also see the BABIP for those drafted to the MLB. It is actually interesting to see how a lot of players in fact fall below one standard deviation from the mean. It doesn’t deem a player a horrible hitter, but he may not be as adept at putting balls in play.\n\nKyle Lewis and Ryan Scott seem to be fantastic pickups by Seattle and Boston. Ryan Scott was picked up in the 7th round, could be a steal there. Lewis was a first rounder, so there are some expectations there.\nI just want to give Nathan a shout out for looking over my grammar, Thanks!"
  },
  {
    "objectID": "posts/tamu_baseball/index.html#source-code",
    "href": "posts/tamu_baseball/index.html#source-code",
    "title": "TAMU BASEBALL",
    "section": "Source Code",
    "text": "Source Code\nAnalysis\nData Scrape"
  },
  {
    "objectID": "posts/tamu_basketball/index.html",
    "href": "posts/tamu_basketball/index.html",
    "title": "SEC non-conference play",
    "section": "",
    "text": "The above chart should provide enough details about schedules and team results during the non-conference schedule. Let’s take a look at the pace of the SEC."
  },
  {
    "objectID": "posts/tamu_basketball/index.html#pace",
    "href": "posts/tamu_basketball/index.html#pace",
    "title": "SEC non-conference play",
    "section": "Pace",
    "text": "Pace\nLet’s see what pace tell us about the SEC. Here is a look at the points per possession and defensive points per possession. There is no true dominant team on both sides of the ball. It looks as though the SEC will truly be a toss-up this year.\n\nEven though no one stands out at the end of non-conference play that could be a result of various factors from injuries, suspensions, type of opponents faced, etc. It’ll definitely evolve as we move on through the season. Now let’s check out how teams are shooting!"
  },
  {
    "objectID": "posts/tamu_basketball/index.html#shooting",
    "href": "posts/tamu_basketball/index.html#shooting",
    "title": "SEC non-conference play",
    "section": "Shooting",
    "text": "Shooting\n\nI’m a bit surprised at Kentucky’s low 3 point attempt rate to start the season. Definitely some concern for South Carolina and Vandy as they struggled to shoot (eFG below 50%) in non-conference play."
  },
  {
    "objectID": "posts/tamu_basketball/index.html#usage-rate",
    "href": "posts/tamu_basketball/index.html#usage-rate",
    "title": "SEC non-conference play",
    "section": "Usage Rate",
    "text": "Usage Rate\n\n\nCollin Sexton is a unique talent, and it definitely can be seen here. His usage rate doesn’t seem to dip with more minutes. Additionally, one of the many freshman (TJ Starks) on the Aggie roster is making the most of his limited playing time. Note I’ve filtered out players who played less than 40 minutes.\nGo ahead and look through for your team’s players.\nLet’s dig a little bit deeper and take a look at each individual team. Let’s look at where the Offense come from?\nI’ve been working on developing assist maps for a while now, but would like to give a shout out to the Yale Sports Group. These assist maps were based of theirs.\nJust a few quick takeaways:\n* Sexton ranks 3rd for weighted assists on Alabama, was thinking he’d be higher.\n* Florida’s offense unsurprisingly runs through Chiozza\n* MSU’s offense seems dependent on Weatherspoon (Guess he’s good? I don’t watch much MSU)\n\nAlabama\n\n\n\nArkansas\n\n\n\nAuburn\n\n\n\nFlorida\n\n\n\nGeorgia\n\n\n\nKentucky\n\n\n\nLSU\n\n\n\nMississippi State\n\n\n\nMissouri\n\n\n\nOle Miss\n\n\n\nSouth Carolina\n\n\n\nTennessee\n\n\n\nTexas A&M\n\n\n\nVanderbilt"
  },
  {
    "objectID": "posts/data_viz_portfolio/index.html",
    "href": "posts/data_viz_portfolio/index.html",
    "title": "Data Viz Portfolio",
    "section": "",
    "text": "Just wanted to share some data visualizations, I’ve created."
  },
  {
    "objectID": "posts/data_viz_portfolio/index.html#sofifa-league-viz-feb-2023",
    "href": "posts/data_viz_portfolio/index.html#sofifa-league-viz-feb-2023",
    "title": "Data Viz Portfolio",
    "section": "SOFIFA League Viz (Feb 2023)",
    "text": "SOFIFA League Viz (Feb 2023)\n\n\nReproducible R Code."
  },
  {
    "objectID": "posts/data_viz_portfolio/index.html#wnba-shot-chart-viz-jan-2023",
    "href": "posts/data_viz_portfolio/index.html#wnba-shot-chart-viz-jan-2023",
    "title": "Data Viz Portfolio",
    "section": "WNBA Shot-Chart viz (Jan 2023)",
    "text": "WNBA Shot-Chart viz (Jan 2023)\n\n\n\nReproducible Code: To be shared"
  },
  {
    "objectID": "posts/data_viz_portfolio/index.html#austin-airport-data-2017",
    "href": "posts/data_viz_portfolio/index.html#austin-airport-data-2017",
    "title": "Data Viz Portfolio",
    "section": "Austin Airport Data (2017)",
    "text": "Austin Airport Data (2017)\nStudying delay patterns out of Austin-Bergstorm.\n\n\nReproducible R Code."
  },
  {
    "objectID": "posts/data_viz_portfolio/index.html#properties-viz-2017",
    "href": "posts/data_viz_portfolio/index.html#properties-viz-2017",
    "title": "Data Viz Portfolio",
    "section": "Properties Viz (2017)",
    "text": "Properties Viz (2017)\n\nComparing green (eco-friendly) properties vs regular properties with respect to the different types of buildings (class). Classes are linked with the age of the property.\nReproducible R Code"
  },
  {
    "objectID": "posts/data_viz_portfolio/index.html#cars-viz-2017",
    "href": "posts/data_viz_portfolio/index.html#cars-viz-2017",
    "title": "Data Viz Portfolio",
    "section": "Cars Viz (2017)",
    "text": "Cars Viz (2017)\n\nReproducible R Code"
  },
  {
    "objectID": "posts/weatheR/index.html",
    "href": "posts/weatheR/index.html",
    "title": "Tri-Cities, WA weatheR",
    "section": "",
    "text": "I knew when moving to Washington/the Tri-cities that while this was going to be the furthest north I had ever lived, it was also a desert. Thank god for the warmth. I interned up here in the summer of 2015, and had been up here for a week or two in the winter. However, I had never experienced a full Washington winter.\nWith my luck, this winter was the “worst” winter in 20 some odd years. Or so everyone claimed. A lot of people claimed that it was bad because it never warmed up fast enough for the snow to melt off. Oh you are damn right it didn’t. On top of that, right as the snow would melt off we would get more. I had to buy SNOW BOOTS to keep my feet warm outside.\nIn early January, I decided I was going to validate these claims about it being one of the worst winters in the Tri-Cities. This had almost slipped my mind, but thankfully a few other rweekly posts reminded me that I needed to finish this. Let’s take a look.\nNote: For weather data see either the NCDC or CDIAC (ORNL)."
  },
  {
    "objectID": "posts/weatheR/index.html#average-temperature",
    "href": "posts/weatheR/index.html#average-temperature",
    "title": "Tri-Cities, WA weatheR",
    "section": "Average Temperature",
    "text": "Average Temperature\nHere is the temperature distribution and how it has changed over the last four years.\n\nSo looking at the past few years up until 2016, there is a gradual decrease near the end of the year into the winter months. However, this past year it seems that there has been a drastic temperature drop into the winter months. It’s also important to note that it was warm for a bit longer than normal.\nI’ve thought a few times about plotting the average temperatures per month, over the course of the last 4 years. Sadly that would be misleading. Since the Tri-Cities is in a desert, the temperature drops drastically once the sun sets. We can however look at the average temperature difference per month.\n\nIn the last two years there has been a drastic drop in temperature swings from September to November. However from 2013 to 2014, the temperature swings took gradual drops from September onwards.\nI am not claiming to be a meteorologist, but it is well known that during the winter months, there is a shorter period of sunlight. Thus, it intuitively makes sense for there to be a smaller temperature difference during the winter months than the rest of the year.\n\nTufte Themed Weather Plot\nEdward Tufte is famous for his work in data visualization. If you haven’t heard of him, I suggest checking his work out. You’ll see very shortly why his work is great.\nIn my search for how to visualize these weather plots, I ran into Brad Boehmke’s work on replicating a Tufte visualization. This was extremely helpful in replicating the tufte plots.\n\nWell, look at that. I’m not really sure if there is much for me to say since the plot does most of the talking. I can’t stress enough the importance of reading a LEGEND. DO IT!"
  },
  {
    "objectID": "posts/weatheR/index.html#snow",
    "href": "posts/weatheR/index.html#snow",
    "title": "Tri-Cities, WA weatheR",
    "section": "Snow",
    "text": "Snow\nTo think I was looking forward to the snow when I first moved up here. It’s not fun to deal with, especially when a lot of snow is on the ground.\nThese heatmaps were inspired by Bob Rudis’s work.\n\nWell looks like the snowfall this past winter was unprecedented. The last time we received a similar snowfall was in the 80’s.\nNote: I’ve double checked some of the years in between to see whether there was any snowfall. I haven’t checked every single year, but am under the assumption that the data sources are correct.\n\nWell as of ’06, it seems like there has been snowfall every winter (excluding 2013). There has been less than 5 inches of snow per month from ’12 onwards.\n\nThe above plot provides a little bit more detail and shows us the total snowfall in the Fall and Winter of respective years. That about sums it up, this was the worst winter in terms of total snowfall accumulated.\nNote: December of the previous year is considered in the Winter of the next year. For example December 2014 is part of the 2015 Winter (Dec ’14, Jan ’15, and Feb ’15)."
  },
  {
    "objectID": "posts/weatheR/index.html#precipitation",
    "href": "posts/weatheR/index.html#precipitation",
    "title": "Tri-Cities, WA weatheR",
    "section": "Precipitation",
    "text": "Precipitation\nI have the precipitation data and am a little curious to see how much rain we actually get. I felt like it rained a bit more than I expected this past year. Let’s see.\n\nWell the Tri-cities really do live up to the hype of being a desert. Less than 10 inches of rain in 36 of the last 42 years. Thank god for the Columbia, Snake and Yakima rivers. It definitely keeps the area arable. The APPLES here are AMAZING. You must try one. Take that for great soil!\nHere is a short summary of the cumulative rainfall and how this past year stacks up historically.\n\nWell compared to previous years, it was a pretty wet one. 2016 ranks as the 8th wettest year dating back to 1975, and the 4th wettest dating back to 1998. Any year where the Tri-cities gets more than 10 inches of a year is definitely something to celebrate.\nFor those of you wondering, why the East side of the state doesn’t get as much rain as the West side it’s because of the rain shadow (that’s what my friend told me at least) that the mountains create. If you ever get the chance to visit Washington, hopefully you get to visit both sides of the state. They are vastly different culturally and climatically."
  },
  {
    "objectID": "posts/weatheR/index.html#references",
    "href": "posts/weatheR/index.html#references",
    "title": "Tri-Cities, WA weatheR",
    "section": "References",
    "text": "References\nThis isn’t an academic paper, but I definitely want to give a shout out to some posts I found online that were helpful.\n\nBrad Boehmke’s work\nBob Rudis’s work\nJoshua Kunst’s work\nSO Question\n\nR Code"
  },
  {
    "objectID": "posts/mid_major_stock_exchange/index.html",
    "href": "posts/mid_major_stock_exchange/index.html",
    "title": "Mid-Major Stock Exchange",
    "section": "",
    "text": "With the start of CBB season last week, the SB Nation blog covering Mid-Majors (Mid-Major Madness) introduced a new fantasy game, Mid-Major Stock Exchange. The premise, similar to the stock market, is to create a portfolio with $500.\nI’ll be honest, I don’t watch a lot of mid-major basketball. Mostly just nationally televised games and the Aggies. This seemed like a prime chance to flex Linear and Integer Programming. Linear and Integer Programming is used in many industries to focus on resource allocation, the goal is to find optimal solutions.\nIn my case, I wanted to optimize returns on mid-major teams. Here is where I encountered the first problem, this is the pre-season and we have not had any games (or previous history in this game) to identify returns. However, if I can correlate team rankings (created from analytics methods) with the cost set it’s possible to estimate potential returns.\n\nTeam Ranking-Cost Correlations\nIf anyone’s interested, I’d definitely look at @brian_lefevre1 work. I used a variation of his pre-season ranking system. First let’s take a look at how the rankings compare to the cost set.\n\nThe rankings and the cost are similar enough, that I’m confident with using the standard errors of the team rankings as part of a random walk to estimate the cost in the future. The team rankings are derived from a linear regression, estimating the adjusted efficiency margins.\n\n\nEstimating Future Costs\nRandom Walk forecasting looks something like this:\n\\[\\begin{align*}\ny\\_{1} = y\\_{0} + e\\_t \\\\\ny_{1} = y_{0} + e_t\n\\end{align*}\\]\nIn our case, cost will be estimated as follows\n\\[\\begin{align*}\nC_{1} = C_{0} (1+\\mathcal{N}(\\mu,\\sigma)) \\\\\nC_{2} = C_{1} (1+\\mathcal{N}(\\mu,\\sigma))\n\\end{align*}\\]\nThe rowwise function in dplyr is key here, otherwise we would use the same random number to estimate future costs for these teams. Later when determining the returns, the differences would all be the same.\nThere are a few teams that have just joined the D-1 ranks or for some reason data is missing. For this, I imputed values based on similar teams. Similarity was defined by teams who had a similar cost (within a 5% margin). The code for this can be seen below.\necdf_fun <- function(x,perc) ecdf(x)(perc)\nimpute_values <- function(df){\n  inds <- which(is.na(df$predTeamRk))\n  for(val in inds){\n    cost_search <- df[val,'Cost'] %>% pull()\n    percent_val <- ecdf_fun(sort(df$Cost),cost_search)\n    top_pct <- percent_val + 0.05\n    low_pct <- percent_val - 0.05\n    range_val <- quantile(df$Cost, probs = c(low_pct, top_pct))\n    search_df <- df %>% filter(Cost >= range_val[1] & Cost <= range_val[2]) %>% drop_na()\n    median_val <- median(search_df$predTeamRk)\n    df[val,\"predTeamRk\"] <- median_val\n  }\n  return(df)\n}\n\n\ncombine_df <- impute_values(combine_df)\nset.seed(41)\n\ncombine_df <- combine_df %>% rowwise() %>%\nmutate(\n  ret = Cost * (1+rnorm(1,mean=0,sd=se.val)),\n  ret2 = ret * (1+rnorm(1,mean=0,sd=se.val)),\n  ret3 = ret2 * (1+rnorm(1,mean=0,sd=se.val)),\n  ret4 = ret3 * (1+rnorm(1,mean=0,sd=se.val)),\n  ret5 = ret4 * (1+rnorm(1,mean=0,sd=se.val)),\n  ret6 = ret5 * (1+rnorm(1,mean=0,sd=se.val)),\n  ret7 = ret6 * (1+rnorm(1,mean=0,sd=se.val)),\n  ret8 = ret7 * (1+rnorm(1,mean=0,sd=se.val)),\n  ret9 = ret8 * (1+rnorm(1,mean=0,sd=se.val))\n)\nNow to calculate the returns, we take the difference of the cost matrix.\n##      abilene christian air force    akron\n## [1,]          17.81000  20.49000 23.08000\n## [2,]          17.56942  20.54356 23.47112\n## [3,]          17.59705  20.57641 23.09482\nHere is a snapshot of the return matrix. The median of the returns matrix is used as the main objective function in the integer program.\n##      abilene christian    air force        akron\n## [1,]      -0.013508198  0.002614065  0.016946222\n## [2,]       0.001572699  0.001598822 -0.016032414\n## [3,]      -0.015698671 -0.001535754 -0.008860428\n\n\nLinear/Integer Programming\nFinally to the good stuff. How do I go about allocating which teams to submit in my portfolio. Linear programs focus on maximizing or minimizing an objective function with a set of constraints. It solves for optimal values of the variables such that the objective function is maximized, in this case.\nThe objective function here is to be maximized based on the estimated median returns found earlier. The first constraint is to ensure that the total of the investments (all stocks purchased) should amass to less than or equal to $500. Another common constraint is ensuring that the values (stocks) are integers.\nAdditionally, I added in another constraint to ensure that no more than 10 shares were owned of each team. Since the matrix was a tad large, the lpSolveAPI package is much more robust than lpSolve.\nHere we set up the IP, and provide the names of all the teams such that the output is much easier to read.\nlibrary(lpSolveAPI)\nn = length(ret_mat2)\nfund_allocate <- make.lp(0,n)\nyNames <- names(ret_mat2)\ncolnames(fund_allocate) <- c(yNames)\n\nObjective\nHere we set the objective\nset.objfn(fund_allocate,c(matrix(ret_mat2, ncol=1, byrow=TRUE)))\n\n\nConstraints\nNow come the constraints\ncosts <- combine_df %>% pull(Cost)\nadd.constraint(fund_allocate,costs,\"<=\",500)\nfor(i in 1:n){\n  add.constraint(fund_allocate,1,\"<=\",10,c(i))\n}\nlp.control(fund_allocate,sense='max')\nset.type(fund_allocate, 1:ncol(fund_allocate), type = 'integer')\nwrite.lp(fund_allocate,'funds.lp',type = 'lp')\nNot to add a few final touches. Ensuring that the objective function is maximized (we don’t want to minimize our returns…..) and to treat this as an integer program (only whole stocks).\nI ended up submitting two portfolios, I made a mistake in my coding, happens to everyone. I accidentally left the problem type set to binary. It was kind of a nice mistake because this allowed me to see what a more diverse portfolio would be if I only invested in each team once.\nA final note I wrote the LP to a text file. It’s helpful to go back and check the text file because the linear program is written out in a clean format.\n\n\n\nResults\nHere is the suggestion on who to invest in for my main portfolio.\nstatus = solve(fund_allocate)\nprint(get.objective(fund_allocate))\n[1] 9.554434\nsolution = get.variables(fund_allocate)\n\nnames <- yNames[which(solution!=0)]\nshares <- solution[which(solution!=0)]\ncost <- costs[which(solution!=0)]\n\ncost_df <- data.frame(Team=names,Shares=shares,Cost=cost)\ncost_df <- cost_df %>% mutate(total_cost = cost*shares)\ncost_df %>%  kable(format=\"markdown\")\n\n\n\nTeam\nShares\nCost\ntotal_cost\n\n\n\n\ndelaware state\n9\n1.25\n11.25\n\n\ndrake\n3\n19.82\n59.46\n\n\nflorida atlantic\n10\n13.56\n135.60\n\n\nnevada\n6\n48.94\n293.64\n\n\n\nHere is the suggestion on who to invest in for my diverse portfolio.\n\n\n\nTeam\nShares\nCost\ntotal_cost\n\n\n\n\narkansas pine bluff\n1\n7.07\n7.07\n\n\nbyu\n1\n36.14\n36.14\n\n\ndelaware state\n1\n1.25\n1.25\n\n\ndrake\n1\n19.82\n19.82\n\n\nflorida atlantic\n1\n13.56\n13.56\n\n\nflorida gulf coast\n1\n21.01\n21.01\n\n\ngrand canyon\n1\n25.57\n25.57\n\n\nincarnate word\n1\n7.46\n7.46\n\n\njacksonville\n1\n15.36\n15.36\n\n\nmiddle tennessee state\n1\n25.63\n25.63\n\n\nmissouri state\n1\n21.90\n21.90\n\n\nmurray state\n1\n27.50\n27.50\n\n\nnevada\n1\n48.94\n48.94\n\n\nnorth dakota state\n1\n20.69\n20.69\n\n\nnorth texas\n1\n24.50\n24.50\n\n\noral roberts\n1\n18.79\n18.79\n\n\nprairie view a&m\n1\n13.65\n13.65\n\n\nst peters\n1\n21.94\n21.94\n\n\nst francis pa\n1\n24.36\n24.36\n\n\numbc\n1\n18.91\n18.91\n\n\nusc upstate\n1\n10.74\n10.74\n\n\ntexas san antonio\n1\n24.48\n24.48\n\n\nvcu\n1\n27.77\n27.77\n\n\nwagner\n1\n21.71\n21.71\n\n\n\nFull code can be found here."
  },
  {
    "objectID": "posts/sec_baseball_recap_2017/index.html",
    "href": "posts/sec_baseball_recap_2017/index.html",
    "title": "SEC Baseball 2017 Recap",
    "section": "",
    "text": "With College baseball season about 6 days away, it’s time to recap last year and look forward to the upcoming year. 2017 was good for SEC teams in general, culminating with a College World Series (CWS) pitting LSU vs. Florida. Florida eventually went on to win the CWS."
  },
  {
    "objectID": "posts/sec_baseball_recap_2017/index.html#expected-w-l",
    "href": "posts/sec_baseball_recap_2017/index.html#expected-w-l",
    "title": "SEC Baseball 2017 Recap",
    "section": "Expected W-L",
    "text": "Expected W-L\n\n\nThe graph on the left hand side, compares the Pythagorean expectation to their actual win percentage. Teams above the diagonal have performed worse than expected and below the diagonal performed better than expected. The Pythagorean expecation is solely based on Runs Scored and Runs allowed.\nThe right hand side plot helps identify how many 1-run,2-run, 10-run games teams were in. Immediately Florida jumps out, they vastly out-performed their Pyt expectation and not surprisingly were in a lot of one-run games. This could suggest that when Florida lost, they lost big but were able to win the close games. (Just speculation here).\nThe opposite can be seen with Kentucky and Vandy they didn’t meet expectations. We can try to tease some insight out of the run differential, but it seems that both of them played in their fair share of close games as well as blow-outs. Kentucky seems to have been on the winning side of blow-outs but weren’t able to close out the close games, which would explain the difference in the Win % seen.\nLet’s move on to look at each team’s hitting performance."
  },
  {
    "objectID": "posts/sec_baseball_recap_2017/index.html#team-hitting",
    "href": "posts/sec_baseball_recap_2017/index.html#team-hitting",
    "title": "SEC Baseball 2017 Recap",
    "section": "Team Hitting",
    "text": "Team Hitting\nThe last couple of years has shown that the SEC has some very capable bats, as well as arms (Vandy) but we will get to that later. Let’s take a look at some numbers below to get a better sense of how teams are scoring.\n\nThe left side takes a deeper look into whether teams are punching runs through methodically (runners on base, Sac Fly, etc) or just knocking it out of the park. To go along with this, the hit distribution for each time can be seen on the right.\nLSU and Kentucky seem to top both charts offensively. Arkansas is truly surprising in that over 8.8% of their hits were HRs, still seemed to be pretty effective. A run is a run. The Aggies (TAMU) and Vanderbilt on the other hand seemed to take a more balanced approach to bringing runners home. Some thanks ought to be sent out to the two coaching staff’s as they clearly have stressed situation hitting. Or Maybe the parks are contributors, Blue Bell field might just be more suspect to triples than Home runs. Who knows? More analysis would need to be done, but a cursory glance shows the Aggies at a vastly greater rate of triples than anyone in the Conference.\nNo team is complete however, without a good pitching staff. Let’s take a look at pitching staff brought back for each team."
  },
  {
    "objectID": "posts/sec_baseball_recap_2017/index.html#team-pitching",
    "href": "posts/sec_baseball_recap_2017/index.html#team-pitching",
    "title": "SEC Baseball 2017 Recap",
    "section": "Team Pitching",
    "text": "Team Pitching\nThe graph below should be indicative of which class year most innings are coming from for each individual team.\n\nVeteran Staffs: TAMU, Mizzou, Bama, and South Carolina\nThe thinking behind Vandy’s lack of seniors might be the incredible pitching development through the Sophomore and Junior years. They always seem to have ace pitching staffs.\nAlright let’s take a look at some of the player stats."
  },
  {
    "objectID": "posts/sec_baseball_recap_2017/index.html#individual-hitting",
    "href": "posts/sec_baseball_recap_2017/index.html#individual-hitting",
    "title": "SEC Baseball 2017 Recap",
    "section": "Individual Hitting",
    "text": "Individual Hitting\nWe’ll look at the top returning hitters. Most players are eligible for the draft at the completion of their junior year. It seems that more hitters tend to leave their Jr year.\n\nI’ve recently become a fan of rolling average plots. The plot on the left looks at streaky-ness for players on a rolling BA for 3 games. 3-games seemed appropriate since that would highlight whether a player had a big series against one team or another. The dashed line is the batting average over the course of the year for each player. Use this as a measuring stick on how the player is performing. The maroon vertical lines on the x-axis are where players had 1-hit games.\nBraden Shewmake stands out as he slowly deteriorates over the course of the year. Tristan Pompey seemed to have a pretty consistent year with a few big weekends in the middle of the season. There is some volatility expected as players are bound to have highs and lows through the year.\nOn the right, we see the distribution of hits for each player over the year. You can see the volatility of the rolling BAs are much greater with players like Biggers and Duplantis as they have more 0-hit games than the others.\nWhile that was helpful, let’s take a look at plate discipline for both sides"
  },
  {
    "objectID": "posts/sec_baseball_recap_2017/index.html#plate-discipline",
    "href": "posts/sec_baseball_recap_2017/index.html#plate-discipline",
    "title": "SEC Baseball 2017 Recap",
    "section": "Plate Discipline",
    "text": "Plate Discipline\nPlate Discipline normally refers to hitters and looks at their K% vs BB%. This helps identify the smart hitters. We will take the opportunity to also look at it for pitchers. This should show how effective and sometimes how patient pitchers are.\n\nThe bottom right corner, on the left plot, shows the most disciplined hitters in the league. The top left corner, on the right plot, shows the most effective pitchers in the league.\nPitchers want to have high K rates and low BB rates, while hitters want the exact opposite (low K rates and high BB rates).\nWell that wraps it up. Look for some future posts using last seasons data. I’m trying to create a random effects model to look at the talent distribution for returning hitters in the SEC. Anyways, can’t wait for the season to start on Friday!"
  },
  {
    "objectID": "posts/sec_baseball_recap_2017/index.html#interactive-record_plots",
    "href": "posts/sec_baseball_recap_2017/index.html#interactive-record_plots",
    "title": "SEC Baseball 2017 Recap",
    "section": "Interactive record_plots",
    "text": "Interactive record_plots\nA few people have requested the ability to interactive with the plate discipline plots. Here you go.\n\n\n\n\nIf you want to take a look at the data or code, take a look at this github link. If you have any questions feel free to reach out @msubbaiah1."
  },
  {
    "objectID": "posts/cfb_bowl_predictive/index.html",
    "href": "posts/cfb_bowl_predictive/index.html",
    "title": "2017 CFB Bowl Predictions",
    "section": "",
    "text": "Last year at work, I was part of the college bowl pick’em. Nothing new, I used to do the CFB bowl pick’ems with my college roommates and others. But last year, I decided to scrape the data and use different Machine Learning (ML) algorithms to predict winners.\nNote: I treated this as a classification problem, 1 - Win, 0 - Lose. For those curious about the ML algorithms and the parameters selected, there is a follow-up post."
  },
  {
    "objectID": "posts/cfb_bowl_predictive/index.html#bowl-predictions",
    "href": "posts/cfb_bowl_predictive/index.html#bowl-predictions",
    "title": "2017 CFB Bowl Predictions",
    "section": "Bowl Predictions",
    "text": "Bowl Predictions\nPrediction accuracies will be updated as bowl season progresses. I’m going to include 4 different set of predictions: 1. Personal picks, 2. Algorithm 1 - Random forest, 3. Algorithm 2 - XGBoost, 4. Algorithm 3 - Support Vector Machines (SVM)\nLet’s look at what each algorithm thinks is important. These were all the possible stats available for all 128 teams in NCAA-FBS (Division I).\n##  [1] \"3rd Down Conversion Pct\"         \"3rd Down Conversion Pct Defense\"\n##  [3] \"4th Down Conversion Pct\"         \"4th Down Conversion Pct Defense\"\n##  [5] \"Fewest Penalties Per Game\"       \"Fewest Penalty Yards Per Game\"  \n##  [7] \"First Downs Defense\"             \"First Downs Offense\"            \n##  [9] \"Kickoff Returns\"                 \"Net Punting\"                    \n## [11] \"Passing Offense\"                 \"Passing Yards Allowed\"          \n## [13] \"Punt Returns\"                    \"Red Zone Defense\"               \n## [15] \"Red Zone Offense\"                \"Rushing Defense\"                \n## [17] \"Rushing Offense\"                 \"Scoring Defense\"                \n## [19] \"Scoring Offense\"                 \"Team Passing Efficiency\"        \n## [21] \"Team Passing Efficiency Defense\" \"T.O.P\"                          \n## [23] \"Total Defense\"                   \"Total Offense\"                  \n## [25] \"Turnover Margin\""
  },
  {
    "objectID": "posts/cfb_bowl_predictive/index.html#algorithm-1---random-forest-vs.-algorithm-2---xgboost",
    "href": "posts/cfb_bowl_predictive/index.html#algorithm-1---random-forest-vs.-algorithm-2---xgboost",
    "title": "2017 CFB Bowl Predictions",
    "section": "Algorithm 1 - Random Forest vs. Algorithm 2 - XGboost",
    "text": "Algorithm 1 - Random Forest vs. Algorithm 2 - XGboost\nRandom Forests (RF) are an ensemble learning method using decision trees. The model has the capability to select and identify the important variables. XGboost is an extreme gradient boosting method applied to decision trees. Similarly to RF it also has the capability to identify important variables.\nLet’s see what we’ve got here.\n\nBefore we dive into this, the models were run to classify/predict whether the home team wins. Above we see the top 10 stats that each model thinks is important to predict home team wins. They both tend to cover the same spectrum, interestingly enough XGB doesn’t consider the home_turnover_margin in the top 10 variables.\nI’d say overall, it tends to do a good job since it captures (1) turnovers, (2) offensive capability, (3) special teams (field position), and (4) clutch conversions (third down %). You can also argue/point out that it captures defense since turnovers and the number of total yards the other team gains.\nFor Support Vector Machines (Algorithm 3 - SVM), they are a bit more complex, and hence don’t provide straight forward variable importances. If you want some more detail on this, look for the follow up post.\nAnyways let’s take a look at the predictions."
  },
  {
    "objectID": "posts/cfb_bowl_predictive/index.html#predictions",
    "href": "posts/cfb_bowl_predictive/index.html#predictions",
    "title": "2017 CFB Bowl Predictions",
    "section": "Predictions",
    "text": "Predictions\n\n\n\n\n\nAway Team\n\n\nHome Team\n\n\nAlgorithm 1\n\n\nA1 - Confidence\n\n\nAlgorithm 2\n\n\nA2 - Confidence\n\n\nAlgorithm 3\n\n\nA3 - Confidence\n\n\nActual\n\n\n\n\n\n\nTroy\n\n\nNorth Texas\n\n\nNorth Texas\n\n\n0.74\n\n\nNorth Texas\n\n\n0.99\n\n\nNorth Texas\n\n\n0.77\n\n\nTroy\n\n\n\n\nGeorgia St.\n\n\nWestern Ky.\n\n\nWestern Ky.\n\n\n0.02\n\n\nGeorgia St.\n\n\n0.11\n\n\nWestern Ky.\n\n\n0.44\n\n\nGeorgia St.\n\n\n\n\nBoise St.\n\n\nOregon\n\n\nOregon\n\n\n0.81\n\n\nOregon\n\n\n0.99\n\n\nOregon\n\n\n0.66\n\n\nBoise St.\n\n\n\n\nMarshall\n\n\nColorado St.\n\n\nColorado St.\n\n\n0.83\n\n\nMarshall\n\n\n0.88\n\n\nColorado St.\n\n\n0.79\n\n\nMarshall\n\n\n\n\nFla. Atlantic\n\n\nAkron\n\n\nFla. Atlantic\n\n\n0.77\n\n\nFla. Atlantic\n\n\n0.97\n\n\nFla. Atlantic\n\n\n0.71\n\n\nFla. Atlantic\n\n\n\n\nSMU\n\n\nLouisiana Tech\n\n\nSMU\n\n\n0.19\n\n\nSMU\n\n\n0.79\n\n\nLouisiana Tech\n\n\n0.05\n\n\nLouisiana Tech\n\n\n\n\nTemple\n\n\nFIU\n\n\nFIU\n\n\n0.68\n\n\nTemple\n\n\n0.50\n\n\nFIU\n\n\n0.37\n\n\nTemple\n\n\n\n\nUAB\n\n\nOhio\n\n\nOhio\n\n\n0.60\n\n\nOhio\n\n\n0.99\n\n\nOhio\n\n\n0.64\n\n\nOhio\n\n\n\n\nWyoming\n\n\nCentral Mich.\n\n\nCentral Mich.\n\n\n0.36\n\n\nCentral Mich.\n\n\n1.00\n\n\nCentral Mich.\n\n\n0.75\n\n\nWyoming\n\n\n\n\nSouth Fla.\n\n\nTexas Tech\n\n\nSouth Fla.\n\n\n0.30\n\n\nSouth Fla.\n\n\n0.68\n\n\nSouth Fla.\n\n\n0.34\n\n\nSouth Fla.\n\n\n\n\nArmy West Point\n\n\nSan Diego St.\n\n\nArmy West Point\n\n\n0.19\n\n\nArmy West Point\n\n\n0.37\n\n\nArmy West Point\n\n\n0.38\n\n\nArmy West Point\n\n\n\n\nAppalachian St.\n\n\nToledo\n\n\nToledo\n\n\n0.54\n\n\nAppalachian St.\n\n\n0.84\n\n\nToledo\n\n\n0.43\n\n\nAppalachian St.\n\n\n\n\nFresno St.\n\n\nHouston\n\n\nHouston\n\n\n0.80\n\n\nHouston\n\n\n1.00\n\n\nHouston\n\n\n0.73\n\n\nFresno St.\n\n\n\n\nWest Virginia\n\n\nUtah\n\n\nUtah\n\n\n0.39\n\n\nWest Virginia\n\n\n0.98\n\n\nWest Virginia\n\n\n0.07\n\n\nUtah\n\n\n\n\nDuke\n\n\nNorthern Ill.\n\n\nNorthern Ill.\n\n\n0.65\n\n\nNorthern Ill.\n\n\n0.96\n\n\nNorthern Ill.\n\n\n0.45\n\n\nDuke\n\n\n\n\nUCLA\n\n\nKansas St.\n\n\nUCLA\n\n\n0.80\n\n\nUCLA\n\n\n0.99\n\n\nUCLA\n\n\n0.35\n\n\nKansas St.\n\n\n\n\nFlorida St.\n\n\nSouthern Miss.\n\n\nSouthern Miss.\n\n\n0.50\n\n\nSouthern Miss.\n\n\n0.96\n\n\nSouthern Miss.\n\n\n0.49\n\n\nFlorida St.\n\n\n\n\nBoston College\n\n\nIowa\n\n\nBoston College\n\n\n0.37\n\n\nBoston College\n\n\n0.41\n\n\nBoston College\n\n\n0.09\n\n\nIowa\n\n\n\n\nArizona\n\n\nPurdue\n\n\nArizona\n\n\n0.27\n\n\nArizona\n\n\n0.44\n\n\nArizona\n\n\n0.79\n\n\nPurdue\n\n\n\n\nTexas\n\n\nMissouri\n\n\nMissouri\n\n\n0.75\n\n\nMissouri\n\n\n1.00\n\n\nMissouri\n\n\n0.84\n\n\nTexas\n\n\n\n\nVirginia\n\n\nNavy\n\n\nNavy\n\n\n0.90\n\n\nNavy\n\n\n1.00\n\n\nNavy\n\n\n0.92\n\n\nNavy\n\n\n\n\nOklahoma St.\n\n\nVirginia Tech\n\n\nOklahoma St.\n\n\n0.33\n\n\nOklahoma St.\n\n\n0.82\n\n\nOklahoma St.\n\n\n0.43\n\n\nOklahoma St.\n\n\n\n\nStanford\n\n\nTCU\n\n\nTCU\n\n\n0.50\n\n\nTCU\n\n\n0.99\n\n\nTCU\n\n\n0.54\n\n\nTCU\n\n\n\n\nMichigan St.\n\n\nWashington St.\n\n\nWashington St.\n\n\n0.48\n\n\nWashington St.\n\n\n1.00\n\n\nWashington St.\n\n\n0.51\n\n\nMichigan St.\n\n\n\n\nWake Forest\n\n\nTexas A&M\n\n\nTexas A&M\n\n\n0.16\n\n\nWake Forest\n\n\n0.55\n\n\nTexas A&M\n\n\n0.19\n\n\nWake Forest\n\n\n\n\nKentucky\n\n\nNorthwestern\n\n\nNorthwestern\n\n\n0.69\n\n\nNorthwestern\n\n\n0.99\n\n\nNorthwestern\n\n\n0.34\n\n\nNorthwestern\n\n\n\n\nNew Mexico St.\n\n\nUtah St.\n\n\nUtah St.\n\n\n0.52\n\n\nNew Mexico St.\n\n\n0.94\n\n\nUtah St.\n\n\n0.23\n\n\nNew Mexico St.\n\n\n\n\nOhio St.\n\n\nSouthern California\n\n\nSouthern California\n\n\n0.29\n\n\nOhio St.\n\n\n0.50\n\n\nOhio St.\n\n\n0.17\n\n\nOhio St.\n\n\n\n\nLouisville\n\n\nMississippi St.\n\n\nMississippi St.\n\n\n0.26\n\n\nMississippi St.\n\n\n0.97\n\n\nLouisville\n\n\n0.19\n\n\nMississippi St.\n\n\n\n\nIowa St.\n\n\nMemphis\n\n\nMemphis\n\n\n0.39\n\n\nMemphis\n\n\n1.00\n\n\nMemphis\n\n\n0.79\n\n\nIowa St.\n\n\n\n\nWashington\n\n\nPenn St.\n\n\nPenn St.\n\n\n0.01\n\n\nPenn St.\n\n\n1.00\n\n\nPenn St.\n\n\n0.37\n\n\nPenn St.\n\n\n\n\nMiami (FL)\n\n\nWisconsin\n\n\nWisconsin\n\n\n0.82\n\n\nWisconsin\n\n\n1.00\n\n\nWisconsin\n\n\n0.78\n\n\nWisconsin\n\n\n\n\nMichigan\n\n\nSouth Carolina\n\n\nMichigan\n\n\n0.42\n\n\nMichigan\n\n\n0.64\n\n\nMichigan\n\n\n0.04\n\n\nSouth Carolina\n\n\n\n\nAuburn\n\n\nUCF\n\n\nUCF\n\n\n0.01\n\n\nUCF\n\n\n0.98\n\n\nUCF\n\n\n0.14\n\n\nUCF\n\n\n\n\nNotre Dame\n\n\nLSU\n\n\nNotre Dame\n\n\n0.16\n\n\nNotre Dame\n\n\n0.68\n\n\nNotre Dame\n\n\n0.34\n\n\nNotre Dame\n\n\n\n\nOklahoma\n\n\nGeorgia\n\n\nGeorgia\n\n\n0.30\n\n\nOklahoma\n\n\n0.89\n\n\nOklahoma\n\n\n0.13\n\n\nGeorgia\n\n\n\n\nClemson\n\n\nAlabama\n\n\nAlabama\n\n\n0.00\n\n\nAlabama\n\n\n0.99\n\n\nAlabama\n\n\n0.18\n\n\nAlabama\n\n\n\n\n\nNote, the confidence of each prediction is also provided using the probability the model provided in predicting a home win or away win. I’ll update this\nI’m quite unhappy with a few picks: (1) UCLA over Kansas St, I’m a huge Bill Synder fan. (2) TCU over Stanford, picking against the ground game of Stanford? (3) Also the confidence in Bama beating Clemson, scares me. 99% for XGB? JEEZ\nI wish I had some conference and scheduled based statistics as well. I’d love to incorporate SOS, power-5 opponents, etc. There are probably a million different things that can be useful. As I continue this yearly, I’ll look for more statistics to include. Let me know if you think there are any that I should consider!"
  },
  {
    "objectID": "posts/cfb_bowl_predictive/index.html#final-results",
    "href": "posts/cfb_bowl_predictive/index.html#final-results",
    "title": "2017 CFB Bowl Predictions",
    "section": "Final Results",
    "text": "Final Results\nAlgorithm 1 - 16/37 = 43.2%\nAlgorithm 2 - 21/37 = 56.7%\nAlgorithm 3 - 15/37 = 40.5%\nNone of these are truly fantastic. I would like to break the 60% threshold. Just like last year, there’s a lot left to learn going forward. I’ll take a stab at this again for the 2018 bowl season. So long folks!\nCongrats to Alabama! TUAAAAAAAA!"
  },
  {
    "objectID": "posts/cfb_bowl_predictive/index2.html",
    "href": "posts/cfb_bowl_predictive/index2.html",
    "title": "pRedictive models for CFB bowl season",
    "section": "",
    "text": "I’ve always loved college sports, especially college football. Bowl season is arguably the closest thing to March Madness when it comes to building a bracket or in this case participating in a pick’em. I’ve done pick’ems in the past with friends and co-workers. Last year, I ventured out to build and use ML models to predict the outcomes.\nHowever, at the time I didn’t really have as much of an expertise in hyper-parmater tuning or what the pitfalls of many models might be. Having started a Business Analytics program this past year, I’m much more familiar with all my potential ML models and tuning parameters.\nI decided to try to predict outcomes again this year and am going to use this post to describe the process. For those interested in what the predictions look like, you can find them here."
  },
  {
    "objectID": "posts/cfb_bowl_predictive/index2.html#data",
    "href": "posts/cfb_bowl_predictive/index2.html#data",
    "title": "pRedictive models for CFB bowl season",
    "section": "Data",
    "text": "Data\nLet’s first take a look at the data. The data includes\ndim(ind_game_df)\n## [1] 773  25\ncolnames(ind_game_df)\n##  [1] \"away_first_downs\"          \"away_rushing_yds\"         \n##  [3] \"away_pass_yds\"             \"away_tot_offense\"         \n##  [5] \"away_pen_number\"           \"away_pen_yds\"             \n##  [7] \"away_pun_yds\"              \"away_pun_re_yds\"          \n##  [9] \"away_kick_re_yds\"          \"home_first_downs\"         \n## [11] \"home_rushing_yds\"          \"home_pass_yds\"            \n## [13] \"home_tot_offense\"          \"home_pen_number\"          \n## [15] \"home_pen_yds\"              \"home_pun_yds\"             \n## [17] \"home_pun_re_yds\"           \"home_kick_re_yds\"         \n## [19] \"away_third_down_conv_pct\"  \"home_third_down_conv_pct\"\n## [21] \"away_fourth_down_conv_pct\" \"home_fourth_down_conv_pct\"\n## [23] \"away_turnover_margin\"      \"home_turnover_margin\"     \n## [25] \"home_win\"\nThere’s about 20 or so columns making it hard to fit the table on one page, so I’ll just go with the column names above. The training data contains statistics for all Division 1 NCAA games, this includes games against FCS opponents. Data covers 773 games for the 2017 season.\ndim(bowl_games_df)\n## [1] 37 24\nThe bowl game data had the same columns as the training set. Each team was pitted against each other and their stats over the course of the season were predicted on. All the statistics here were averages over the season. This is inconsistent to predict using the averages but build using individual game statistics. At the moment, I’m not sure how to handle this. I’ll address further concerns a bit later.\nNote that not all bowl games are included, data was missing for certain opponents. I predicted the outcome of 37 bowl games, there are 39 bowl games in total + the playoff championship."
  },
  {
    "objectID": "posts/cfb_bowl_predictive/index2.html#methods",
    "href": "posts/cfb_bowl_predictive/index2.html#methods",
    "title": "pRedictive models for CFB bowl season",
    "section": "Methods",
    "text": "Methods\nI decided to run three different models to compare prediction accuracy. The models built are Random Forests, XGBoost and Support Vector Machines (SVMs). I decided to approach this problem as a classification instead of a prediction. I’ll try and keep the model building part short!\nThe original training data was split into a training and validation set to identify the proper parameters for each method. I used a 75-25 split.\nset.seed(22)\nsample <- sample.int(n = nrow(ind_game_df), size = floor(.75*nrow(ind_game_df)), replace = F)\ntrain <- ind_game_df[sample, ]\ntest  <- ind_game_df[-sample, ]\n\ny <- \"home_win\"\nx <- setdiff(names(ind_game_df), y)"
  },
  {
    "objectID": "posts/cfb_bowl_predictive/index2.html#models",
    "href": "posts/cfb_bowl_predictive/index2.html#models",
    "title": "pRedictive models for CFB bowl season",
    "section": "Models",
    "text": "Models\nI’m going to use the h20 package to build my tree based methods. In the past, I’ve found them to be more computationally efficient by leveraging parallel processes.\n\nRandom Forests (RF)\nFor RF, a grid-search approach was taken to identify the optimal mtry, ntree, and min_rows. Optimal values were selected to ensure the highest out-of-sample accuracy.\nlibrary(h2o)\nh2o.init(nthreads = 3, max_mem_size = \"6G\")\n# hide progress bar for markdown\nh2o.no_progress()\ntunegrid <- expand.grid(.mtry=c(12,15,20), .ntree=c(200,500, 700),.min_rows=c(5,7,10))\ntunegrid$acc <- 0\n\ndat_tr <- as.h2o(train)\ndat_ts <- as.h2o(test)\n\nfor(i in 1:nrow(tunegrid)){\n  mtries <- tunegrid$.mtry[i]\n  num_trees <- tunegrid$.ntree[i]\n  min_rows <- tunegrid$.min_rows[i]\n  ptr_statement <- paste0(\"Working on \",i/nrow(tunegrid)*100, \"%\", sep=\" \")\n  #print(ptr_statement)\n  rf_fit1 <- h2o.randomForest(x = x,\n                              y = y,\n                              training_frame = dat_tr,\n                              model_id = \"rf_fit1\",\n                              seed = 1,\n                              ntrees = num_trees,\n                              mtries = mtries,\n                              min_rows = min_rows)\n\n  y_pred <- h2o.predict(rf_fit1,\n                  newdata = dat_ts)\n\n  y_pred_df <- as_data_frame(y_pred)\n  conf_mat <- table(test$home_win,y_pred_df$predict)\n  tunegrid$acc[i] <- sum(diag(conf_mat))/sum(conf_mat)\n}\n\n\nThe best model was found with a minimum of 5 rows in each split. Playing with this parameter allows us to control potential over-fitting by isolating some splits down to exactly one row.\nThe best random forest model was selected with sampling 12 variables mtry, either 500 or 700 trees per forest ntree, and 5 rows per split min_rows. The accuracy here is about 81.4%. So from the 193 games in the testing set, it misclassify wins for 36 games. Not too shabby.\nWhile Random Forests are pretty good, I’ve heard even greater things about gradient boosting trees, let’s check that out next!\n\n\nXGBoost (XGB)\nSimiarly for XGBoost, a grid-search approach was taken to identify the optimal learn_rate, max depth, and booster. Note for classification models, only gbtree and dart are valid booster functions.\nxg_tunegrid <- expand.grid(.learn_rate=seq(0.1,0.9,by=0.2), .booster=c(\"gbtree\",\"dart\"),.max_depth=c(6,10,20,30))\nxg_tunegrid$.booster <- as.character(xg_tunegrid$.booster)\nxg_tunegrid$acc <- 0\n\nfor(i in 1:nrow(xg_tunegrid)){\n  lr <- xg_tunegrid$.learn_rate[i]\n  br <- xg_tunegrid$.booster[i]\n  md <- xg_tunegrid$.max_depth[i]\n  ptr_statement <- paste0(\"Working on \",i/nrow(xg_tunegrid)*100, \"%\", sep=\" \")\n  #print(ptr_statement)\n  rf_fit2 <- h2o.xgboost(x=x,\n                         y=y,\n                         training_frame = dat_tr,\n                         model_id = \"rf_fit1\",\n                         seed = 1,\n                         learn_rate = lr,\n                         booster = br,\n                         max_depth = md)\n\n  y_pred2 <- h2o.predict(rf_fit2,newdata = dat_ts)\n  y_pred_df <- as_data_frame(y_pred2)\n  conf_mat <- table(test$home_win,y_pred_df$predict)\n  xg_tunegrid$acc[i] <- sum(diag(conf_mat))/sum(conf_mat)\n}\n\n\nInterestingly enough, the booster functions didn’t change the out-of-sample accuracies. Instead they were the exact same for every iteration of the learning rate and the tree depth. Therefore, I stuck with the gbtree as that is the default in classification methods.\nThe best model selected here has a learning rate learn_rate of 0.1, and a tree depth max_depth of 6. The accuracy here is about 83.5%. So from the 193 games in the testing set, it misclassify wins for 32 games. A slight improvement from the random forests method.\nDiversity is good and I wanted to move away from the tree models. So let’s check out Support Vector Machines (SVM).\n\n\nSupport Vector Machines (SVM)\nH2o doesn’t support SVM, hence my use of caret. I’ve been meaning to trying caret anyways. I really do wish there was a parallel to sklearn in R. Maybe caret is that?\nLastly for SVM, a grid-search approach was taken to identify the optimal sigma and slack (C)\nlibrary(doParallel)\nregisterDoParallel(cores=3)\ngeomSeries <- function(base, max) {\n  base^(0:floor(log(max, base)))\n}\n\nsvm_grid <- expand.grid(sigma = c(0,0.01, 0.02, 0.025, 0.03, 0.04,\n                          0.05, 0.06, 0.07,0.08, 0.09, 0.1, 0.25, 0.5, 0.75,0.9),\n                              C = sort(unique(c(geomSeries(base=10, max=10^-5),\n                                               geomSeries(base=10, max=10^5)))))\n\nlibrary(caret)\ntrctrl <- trainControl(method = \"repeatedcv\", number = 10, repeats = 3,\n                       classProbs =  TRUE)\n\nind_game_df$home_win <- as.factor(make.names(ind_game_df$home_win))\nsvm_Radial <- train(home_win ~., data = ind_game_df, method = \"svmRadial\",\n                      trControl=trctrl,\n                      preProcess = c(\"center\", \"scale\"),\n                      tuneLength = 10,\n                      tuneGrid = svm_grid)\n### come back and print just the best value\n#svm_Radial\n\n\nThe main parameter to identify for SVMs is slack, since it controls the margins. The tradeoff that we look for in slack is between fitting simple functions and fitting to the data exactly. Since we use RBF for the kernel, we also need to optimize the sigma value.\nThe best model for SVM obtained a C (slack) of 1 and a sigma of 0.01. The accuracy here is about 88%. So from the 193 games in the testing set, it misclassify wins for 23 games. The best so far.\nInstead of just selecting SVM to predict bowl games, I went with all three to see how they perform against each other. To see how the predictions are doing, see the following post.\nThat sums up how the models were put together, but first I’d like to discuss some potential pitfalls. Sadly nothing is ever perfect and I’m still finding ways to improve these methods."
  },
  {
    "objectID": "posts/cfb_bowl_predictive/index2.html#concerns",
    "href": "posts/cfb_bowl_predictive/index2.html#concerns",
    "title": "pRedictive models for CFB bowl season",
    "section": "Concerns",
    "text": "Concerns\nCaveat: I used home and away as just placeholders to allow me to pit teams against each other. There was no in-built\n[To Be finished]"
  },
  {
    "objectID": "posts/getting_betteR/index.html",
    "href": "posts/getting_betteR/index.html",
    "title": "Getting betteR",
    "section": "",
    "text": "I started using R about a year and a half ago at my summer internship with PNNL, also my current employer. Note: Opinions expressed are solely my own and do not express the views or opinions of my employer.\nThat paved the way for me to use R for my master’s thesis. Now, I use R for a variety of things. I use python as well, mostly for web scraping currently. Hopefully, that’ll change. But this is more about the evolution of my R knowledge. So let’s get to it.\nFor the past two years, I’ve created an NCAA stat visualizer. (In the near future I will try to provide predictions for any potential match-up.) The stat visualizer has been handy when filling out brackets. One of the most time intensive processes in this yearly project, is data cleaning and manipulation.\nI’ll highlight a handful of useful R tips including, proper list usage, vectorization, tidyverse libraries, and foreach parallelization. I picked these specific topics, as that’s what I’ve found myself using often when coding in R.\nNote: The scraped data has four different csvs, individual game (team and player), overall (team and player) data."
  },
  {
    "objectID": "posts/getting_betteR/index.html#list-usage",
    "href": "posts/getting_betteR/index.html#list-usage",
    "title": "Getting betteR",
    "section": "List Usage",
    "text": "List Usage\nLet’s start with the proper list usage. Lately, I’ve become a firm believer in using lists when appropriate. The example shown below reads in multiple csvs into a list, and then separate that list into individual data-frames.\nI would’ve liked to benchmark this better, but for now I’m just going to time both options.\nptm <- proc.time()\nagg.player <- read.csv(file=\"data/summary_player_data.tsv\", sep=\"\\t\", header=TRUE, row.names=1, na.strings=\"?\")\nagg.team <- read.csv(file=\"data/summary_team_data.tsv\", sep=\"\\t\", header=TRUE, row.names=1, na.strings=\"?\")\nind.game <- read.csv(file=\"data/game_data.tsv\", sep=\"\\t\", header=TRUE, row.names=1, na.strings=\"?\")\nind.player <- read.csv(file=\"data/player_data.tsv\", sep=\"\\t\", header=TRUE, row.names=NULL, na.strings=\"?\")\nproc.time() - ptm\n##    user  system elapsed\n##   1.272   0.008   1.281\nLet’s take a look at loading all of the csvs into one list.\nptm <- proc.time()\nfile_lists <- list.files(path=\"data\",pattern=\"*.tsv\")\nfile_lists <- paste0(\"data/\",file_lists)\nfile_names <- gsub(\".tsv\",\"\",file_lists)\nmy.data <- lapply(file_lists, read.csv,sep=\"\\t\", header=TRUE, row.names=NULL, na.strings=\"?\")\nnames(my.data) <- file_names\nlist2env(my.data,.GlobalEnv)\n## <environment: R_GlobalEnv>\nproc.time() - ptm\n##    user  system elapsed\n##   1.096   0.012   1.110\nIn this scenario, there is little to no difference in the two options (time-wise). You can load the data any way you want. I prefer the second option, much less typing. :)\nNote: The main advantage of a list is that it can be a collection of varying different elements. For example, a list could store two different dataframes and a vector (with differing structures). Lists are vectors in R. However, Lists are recursive in nature while vectors are not. “Recursive” here refers to the fact that it can contain values of different types, lengths, etc.\nIf you are new to R and trying to learn how the different data types work, here are a few links. Impatient R Tutorials Point Stat Methods SO #1\nPer the request of a friend, I’ll follow up with an article focusing on the R data types."
  },
  {
    "objectID": "posts/getting_betteR/index.html#vectors",
    "href": "posts/getting_betteR/index.html#vectors",
    "title": "Getting betteR",
    "section": "Vectors",
    "text": "Vectors\nR is a vector based language. I didn’t actually know this until I took a class in graduate school, where the professor taught R with a bioinformatics spin. I am very glad I learnt that, and can show you exactly why below.\nThis time I’ll benchmark our calls. Note, benchmarking essentially runs the code as many times as specified and records timing. This provides a general expected range for how long a chunk of code might take. So let’s take a look.\nbasicgamestats <- c(\"fgm\", \"fga\", \"three_fgm\", \"three_fga\", \"ft\", \"fta\", \"pts\", \"ptsavg\", \"offreb\", \"defreb\", \"totreb\", \"rebavg\", \"ast\", \"to\", \"stl\", \"blk\", \"fouls\", \"dbldbl\", \"trpdbl\",\"fgpct\", \"three_fgpct\", \"ftpct\")\nbasicgamestats_team <- basicgamestats\n\nmbm = microbenchmark(\n  s_apply_method = sapply(seq(1:31), function (x){\n  gen_var_name <- basicgamestats_team[x]\n  paste0(\"team_\", gen_var_name)}),\n  vector_method = paste0(\"team_\",basicgamestats_team)\n)\nautoplot(mbm)\n\nWell, now that I (formally) know that R is a vector based language this piece of code is accomplished quickly. (Note, the sapply loop is the old method (not-vectorized) fashion of appending text.) Instead of searching and appending “team_” to each stat via a loop, the paste0 function takes care of it. For further details on vectorization, take a look here\nAlright, lets move forth to the TIDYVERSE! Yep, the more and more I browse stack overflow, answers are provided with a focus on using tidyverse or data.table solutions."
  },
  {
    "objectID": "posts/getting_betteR/index.html#tidyverse",
    "href": "posts/getting_betteR/index.html#tidyverse",
    "title": "Getting betteR",
    "section": "Tidyverse",
    "text": "Tidyverse\nThe tidyverse is a collection of R packages that share common philosophies and are designed to work together, engineered by Hadley Wickham and co. For documentation, go to http://tidyverse.org/. Dplyr is a tidyverse package.\nLooking through the data, there are additional columns that need to be created to determine stat percentages among other things. Originally, I didn’t use good programming practice.\n\nBase R - Column adds\nptm <- proc.time()\nind.game$ptsdiff <- ind.game$home_team_pts - ind.game$away_team_pts\n# Field goal percentage\nind.game$away_team_fgpct <- ind.game$away_team_fgm/ind.game$away_team_fga\nind.game$home_team_fgpct <- ind.game$home_team_fgm/ind.game$home_team_fga\n# Three-point percentage\nind.game$away_team_three_fgpct <- ind.game$away_team_three_fgm/ind.game$away_team_three_fga\nind.game$home_team_three_fgpct <- ind.game$home_team_three_fgm/ind.game$home_team_three_fga\n# Free-throw percentage\nind.game$away_team_ftpct <- ind.game$away_team_ft/ind.game$away_team_fta\nind.game$home_team_ftpct <- ind.game$home_team_ft/ind.game$home_team_fta\nproc.time() - ptm\n##    user  system elapsed\n##   0.008   0.000   0.008\n\n\ntidyverse - Column adds\nptm <- proc.time()\nind.game <- ind.game %>% mutate(\n  ptsdiff = home_team_pts - away_team_pts,\n  away_team_fgpct = away_team_fgm/away_team_fga,\n  home_team_fgpct = home_team_fgm/home_team_fga,\n  away_team_three_fgpct = away_team_three_fgm/away_team_three_fga,\n  home_team_three_fgpct = home_team_three_fgm/home_team_three_fga,\n  away_team_ftpct = away_team_ft/away_team_fta,\n  home_team_ftpct = home_team_ft/home_team_fta\n)\nproc.time() - ptm\n##    user  system elapsed\n##   0.004   0.000   0.006\nWhen it comes to making new columns. There is no real advantage whether you use base R or tidyverse for this. However, I would argue that it is good programming practice to rely on the using mutate (dplyr) or with (base) when trying to create multiple columns. The with function is helpful as it doesn’t force you to keep referencing data.frame$‘column name’ repeatedly, as seen above.\nHere shortly you should see the advantage of using the tidyverse with some of the more complex situations.\n\n\nbase R (sapply) vs. tidyverse\nIn this example, the goal is to calculate the total number of fouls accrued per player. This is done by searching through each individual game and summing the fouls. After it is calculated it is then appended back to the aggregate player data frame.\nSapply approach: Loop through the aggregate player data-frame, takes the player name and finds all games where the players played and sums the foul column.\nTidyverse approach: Group the per game stats by player names and then sum by the foul column. Dplyr has the same set of join methods seen in SQL. The inner join is used to match and merge the data back based on player names.\ntv_mbm = microbenchmark(\nbase_r = sapply(seq(1:nrow(agg.player)),function(i){\n  player_subset <- subset(ind.player,ind.player$player_name==agg.player$player_name[i])\n  return(sum(player_subset$fouls,na.rm=TRUE))\n}),\ndplyr = ind.player %>%  group_by(player_name)  %>% summarise(PF = sum(fouls,na.rm=TRUE)) %>% inner_join(agg.player,.)\n,times = 10)\nautoplot(tv_mbm)\n\nWell, the tidyverse approach is much quicker. Good to know that things are moving along in the right direction.\nAs you can see, this plot differs a little from the previous violin plot and that is mostly a result of the number of data points. Microbenchmark has a parameter that dictates the number of times a chunk of code is to be repeated. The default is 100, but in this case I have set it 10 (otherwise it would take me an hour to generate the results, YIKES)."
  },
  {
    "objectID": "posts/getting_betteR/index.html#parallel-processing",
    "href": "posts/getting_betteR/index.html#parallel-processing",
    "title": "Getting betteR",
    "section": "Parallel Processing",
    "text": "Parallel Processing\nAfter doing some reading online, I quickly realized that using foreach loops would have the ability to significantly improve performance.\nOne of the most time consuming calculations was aggregating season data for both home and away games.\nnprl_time <- proc.time()\nind.game.games <- ind.game$game_id\n\nfor (variable in seq(along=home_away_vars_to_populate)) {\n  gen_var_name <- home_away_vars_to_populate[variable]\n  away_command <- paste0(\"away_season_\", gen_var_name)\n  home_command <- paste0(\"home_season_\", gen_var_name)\n  assign(away_command, sapply(seq(1:length(ind.game$game_date)),function(x){\n    gen_game_id <- ind.game.games[x]\n    gen_away_team_id <- as.character(ind.game[gen_game_id, ]$away_team_id)\n    tm_sub <- agg.team[gen_away_team_id,]\n    tm_sub[,gen_var_name]\n  }))\n  assign(home_command, sapply(seq(1:length(ind.game$game_date)),function(x){\n    gen_game_id <- ind.game.games[x]\n    gen_home_team_id <- as.character(ind.game[gen_game_id, ]$home_team_id)\n    tm_sub <- agg.team[gen_home_team_id,]\n    tm_sub[,gen_var_name]\n  }))\n}\nn_prl_time <- proc.time() - nprl_time\nSo looking back, this is a pretty inefficient chunk of code. It stores the data into separate vectors, which have to be appended back into the data. A nested sapply loop within a for loop definitely slows things down. Also, if you are interested in using the row names of a data-frame then consider saving it as a separate column.\nThe advantage of parallelizing code is that the code is split into pieces, executed in parallel and then the results are combined together. There are a a plethora of parallel packages in R. I will post a set of links below, if you are interested in learning more about parallelization in R.\nEnough talk, here is the parallel code.\nlibrary(parallel)\nlibrary(doSNOW)\nnocores <- detectCores() - 1\ncl <- makeCluster(nocores)\nregisterDoSNOW(cl)\n\np_time <- proc.time()\naway_var_list <- foreach(i=1:31,.combine=data.frame, .packages = \"dplyr\") %dopar% {\n  gen_var_name <- home_away_vars_to_populate[i]\n  sapply(seq(1:length(ind.game$game_date)),function(x){\n    gen_away_team_id <- as.character(ind.game$away_team_name[x])\n    tm_sub <- agg.team %>% filter(team_name == gen_away_team_id)\n    ifelse(nrow(tm_sub) == 0,-999,tm_sub[,gen_var_name])\n  })\n}\n\nhome_var_list <- foreach(i=1:31,.combine=data.frame, .packages = \"dplyr\") %dopar% {\n  gen_var_name <- home_away_vars_to_populate[i]\n  sapply(seq(1:length(ind.game$game_date)),function(x){\n    gen_home_team_id <- as.character(ind.game$home_team_name[x])\n    tm_sub <- agg.team %>% filter(team_name == gen_home_team_id)\n    ifelse(nrow(tm_sub) == 0,-999,tm_sub[,gen_var_name])\n  })\n}\npar_time <- proc.time() - p_time\nstopCluster(cl)\n## Regular Code\nn_prl_time\n##    user  system elapsed\n## 979.632   0.128 979.957\n## Parallel Code\npar_time\n##    user  system elapsed\n##   0.216   0.008 574.619\n## Code improvement\nn_prl_time[3]/par_time[3]\n##  elapsed\n## 1.705403\nFor a little insight on the three different times shown, I went to the documentation for proc.time.\nAs per the documentation: The ‘user time’ is the CPU time charged for the execution of user instructions of the calling process. The ‘system time’ is the CPU time charged for execution by the system on behalf of the calling process.\nElapsed time is easily perceived to the user (i.e when your code is done running), we use it to determine the performance impact from a time standpoint. It’s about 2x times faster. Definitely worth it, in my opinion."
  },
  {
    "objectID": "posts/getting_betteR/index.html#tldr",
    "href": "posts/getting_betteR/index.html#tldr",
    "title": "Getting betteR",
    "section": "TLDR",
    "text": "TLDR\nSo in short, there is a time for base r, tidyverse, and even parallel processing. Hopefully, this post highlighted the performance improvement (from a time standpoint) when used correctly.\nI’ve hit on some R packages and features that you will find useful. As you can see, base R does a lot of things well. However when it comes to the harder data munging and cleaning tasks it falls short. At first I thought about using for loops, but quickly learned that I should rely on apply family of loops. Turns out some of the tasks I was trying to accomplish are easily handled by the tidyverse set of packages, specifically dplyr. I was a bit surprised that I had to rely on parallel processing to sort through the data and determine opponent statistics. It proved to be very time efficient and satisfied the computer nerd in me.\nI’m sure I will follow this post up again (in a year) detailing some other cool packages and features available in R. Data.table is on the docket of things to learn. Let me know if you have any other recommendations. Tweet at @msubbaiah1."
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Data Viz Portfolio\n\n\n\n\n\n\n\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\nHeroku for Data\n\n\n\n\n\n\n\n\n\nMar 14, 2019\n\n\n\n\n\n\n\n\nMid-Major Stock Exchange\n\n\n\n\n\n\n\n\n\nNov 11, 2018\n\n\n\n\n\n\n\n\nSEC Baseball 2017 Recap\n\n\n\n\n\n\n\n\n\nFeb 11, 2018\n\n\n\n\n\n\n\n\nTri-Cities, WA weatheR\n\n\n\n\n\n\n\n\n\nFeb 11, 2018\n\n\n\n\n\n\n\n\nSEC non-conference play\n\n\n\n\n\n\n\n\n\nJan 11, 2018\n\n\n\n\n\n\n\n\n2017 CFB Bowl Predictions\n\n\n\n\n\n\n\n\n\nDec 20, 2017\n\n\n\n\n\n\n\n\npRedictive models for CFB bowl season\n\n\n\n\n\n\n\n\n\nDec 20, 2017\n\n\n\n\n\n\n\n\nGetting betteR\n\n\n\n\n\n\n\n\n\nMar 26, 2017\n\n\n\n\n\n\n\n\nClustering CFB Offensive Styles\n\n\n\n\n\n\n\n\n\nOct 29, 2016\n\n\n\n\n\n\n\n\nTAMU BASEBALL\n\n\n\n\n\n\n\n\n\nJun 15, 2016\n\n\n\n\n\n\n\n\nCollege Shiny Apps\n\n\n\n\n\n\n\n\n\nMar 3, 2016\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nData Viz Portfolio\n\n\n\n\n\n\n\nData Viz\n\n\nR\n\n\n\n\nJust a place to showcase and get better at data-viz\n\n\n\n\n\n\nJan 1, 2023\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeroku for Data\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\nHeroku\n\n\n\n\nAutomate Tweets with Heroku\n\n\n\n\n\n\nMar 14, 2019\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nMid-Major Stock Exchange\n\n\n\n\n\n\n\ncollege basketball\n\n\nR\n\n\n\n\nInvesting in CBB mid-major teams\n\n\n\n\n\n\nNov 11, 2018\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nSEC Baseball 2017 Recap\n\n\n\n\n\n\n\ncollege baseball\n\n\nR\n\n\npython\n\n\n\n\nBreaking down the 2017 baseball season for SEC teams\n\n\n\n\n\n\nFeb 11, 2018\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nTri-Cities, WA weatheR\n\n\n\n\n\n\n\nR\n\n\nData Viz\n\n\n\n\nWeather Viz in R\n\n\n\n\n\n\nFeb 11, 2018\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nSEC non-conference play\n\n\n\n\n\n\n\nR\n\n\ncollege basketball\n\n\n\n\nLet’s take a look around the SEC and how everyone’s non-conference schedule played out\n\n\n\n\n\n\nJan 11, 2018\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\n2017 CFB Bowl Predictions\n\n\n\n\n\n\n\ncollege football\n\n\nR\n\n\npython\n\n\n\n\nCFB Bowl Predictions\n\n\n\n\n\n\nDec 20, 2017\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npRedictive models for CFB bowl season\n\n\n\n\n\n\n\ncollege football\n\n\nR\n\n\npython\n\n\n\n\nCFB model building\n\n\n\n\n\n\nDec 20, 2017\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nGetting betteR\n\n\n\n\n\n\n\nR\n\n\n\n\nDocumenting my slow learnings and improvements with R\n\n\n\n\n\n\nMar 26, 2017\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nClustering CFB Offensive Styles\n\n\n\n\n\n\n\nR\n\n\ncollege football\n\n\n\n\nCFB Styles of Play Analysis\n\n\n\n\n\n\nOct 29, 2016\n\n\n36 min\n\n\n\n\n\n\n  \n\n\n\n\nTAMU BASEBALL\n\n\n\n\n\n\n\nR\n\n\ncollege baseball\n\n\n\n\nTexas A&M and their powerful bats. Analysis on Texas A&M Baseball 2015-2016 season\n\n\n\n\n\n\nJun 15, 2016\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nCollege Shiny Apps\n\n\n\n\n\n\n\nShiny\n\n\nR\n\n\ncollege basketball\n\n\ncollege baseball\n\n\n\n\nShiny apps visualizing March Madness/CWS\n\n\n\n\n\n\nMar 3, 2016\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Meyappan",
    "section": "",
    "text": "About this blog"
  }
]